{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maumau-3005/Hydra-fullstack/blob/main/webscrapping_hydra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "empresa_header.py — monta JSON com nome da empresa e setor GICS a partir de um ticker e 2 datas.\n",
        "\n",
        "Exemplo de retorno:\n",
        "{\n",
        "    \"codigo_da_ação\": \"PETR3\",\n",
        "    \"Nome_da_empresa\": \"Petrobras - Petróleo Brasileiro S.A.\",\n",
        "    \"Setor_da_empresa\": \"Energy\",\n",
        "    \"Final_do_teste\": \"2025-06-30\",\n",
        "    \"hojé_em_dia\": \"2025-09-06\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import warnings\n",
        "from typing import Dict, Optional, List\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "try:\n",
        "    import yfinance as yf  # pip install yfinance\n",
        "except Exception:  # pragma: no cover\n",
        "    yf = None\n",
        "\n",
        "\n",
        "# ---------------------- Normalização GICS ----------------------\n",
        "_GICS_SET = {\n",
        "    \"Communication Services\",\n",
        "    \"Consumer Discretionary\",\n",
        "    \"Consumer Staples\",\n",
        "    \"Energy\",\n",
        "    \"Financials\",\n",
        "    \"Health Care\",\n",
        "    \"Industrials\",\n",
        "    \"Information Technology\",\n",
        "    \"Materials\",\n",
        "    \"Real Estate\",\n",
        "    \"Utilities\",\n",
        "}\n",
        "\n",
        "# mapeia variações/comuns -> GICS oficial\n",
        "_GICS_NORMALIZE = {\n",
        "    \"communication services\": \"Communication Services\",\n",
        "    \"communications\": \"Communication Services\",\n",
        "    \"consumer discretionary\": \"Consumer Discretionary\",\n",
        "    \"consumer staples\": \"Consumer Staples\",\n",
        "    \"energy\": \"Energy\",\n",
        "    \"financials\": \"Financials\",\n",
        "    \"financial services\": \"Financials\",\n",
        "    \"health care\": \"Health Care\",\n",
        "    \"healthcare\": \"Health Care\",\n",
        "    \"industrials\": \"Industrials\",\n",
        "    \"industrial\": \"Industrials\",\n",
        "    \"information technology\": \"Information Technology\",\n",
        "    \"technology\": \"Information Technology\",\n",
        "    \"tech\": \"Information Technology\",\n",
        "    \"materials\": \"Materials\",\n",
        "    \"real estate\": \"Real Estate\",\n",
        "    \"real-estate\": \"Real Estate\",\n",
        "    \"utilities\": \"Utilities\",\n",
        "    # pt/pt-br ocasionais\n",
        "    \"tecnologia\": \"Information Technology\",\n",
        "    \"saúde\": \"Health Care\",\n",
        "    \"energia\": \"Energy\",\n",
        "    \"financeiro\": \"Financials\",\n",
        "    \"materiais\": \"Materials\",\n",
        "    \"utilidades\": \"Utilities\",\n",
        "    \"bens de consumo\": \"Consumer Staples\",\n",
        "    \"discricionário\": \"Consumer Discretionary\",\n",
        "    \"comunicação\": \"Communication Services\",\n",
        "    \"imobiliário\": \"Real Estate\",\n",
        "}\n",
        "\n",
        "def _normalize_gics(sector: Optional[str]) -> Optional[str]:\n",
        "    if not sector or not isinstance(sector, str):\n",
        "        return None\n",
        "    s = sector.strip()\n",
        "    if s in _GICS_SET:\n",
        "        return s\n",
        "    key = s.lower()\n",
        "    return _GICS_NORMALIZE.get(key, None)\n",
        "\n",
        "\n",
        "# ---------------------- Heurísticas de símbolo ----------------------\n",
        "def _guess_yahoo_variants(code: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Gera variantes prováveis para o Yahoo Finance.\n",
        "    - B3: código alfanumérico terminado em dígito (ex.: PETR3, VALE3, ITUB4) -> '.SA'\n",
        "    - Caso já contenha um sufixo (.SA, .NY, .US etc.), usa como está e também tenta sem.\n",
        "    - Em geral: tenta [code, code+'.SA'] nessa ordem, sem duplicar.\n",
        "    \"\"\"\n",
        "    code = (code or \"\").strip().upper()\n",
        "    variants: List[str] = []\n",
        "    has_dot = \".\" in code\n",
        "    # como está\n",
        "    variants.append(code)\n",
        "    # heurística B3\n",
        "    if not has_dot:\n",
        "        # padrão B3 clássico: 4 letras + 1-2 dígitos (3,4,11 etc.)\n",
        "        import re\n",
        "        if re.fullmatch(r\"[A-Z]{4}\\d{1,2}\", code):\n",
        "            variants.append(f\"{code}.SA\")\n",
        "    # se usuário já passou .SA, tentamos também sem\n",
        "    if has_dot and code.endswith(\".SA\"):\n",
        "        base = code.split(\".\")[0]\n",
        "        variants.append(base)\n",
        "    # remove duplicados mantendo a ordem\n",
        "    out: List[str] = []\n",
        "    for v in variants:\n",
        "        if v not in out:\n",
        "            out.append(v)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------- Fallback local (nomes/sectores) ----------------------\n",
        "# Pequeno dicionário para casos comuns se yfinance falhar.\n",
        "_FALLBACK_PROFILE = {\n",
        "    # B3\n",
        "    \"PETR3\": (\"Petrobras - Petróleo Brasileiro S.A.\", \"Energy\"),\n",
        "    \"PETR4\": (\"Petrobras - Petróleo Brasileiro S.A.\", \"Energy\"),\n",
        "    \"VALE3\": (\"Vale S.A.\", \"Materials\"),\n",
        "    \"ITUB4\": (\"Itaú Unibanco Holding S.A.\", \"Financials\"),\n",
        "    \"BBDC4\": (\"Banco Bradesco S.A.\", \"Financials\"),\n",
        "    \"ABEV3\": (\"Ambev S.A.\", \"Consumer Staples\"),\n",
        "    \"BBAS3\": (\"Banco do Brasil S.A.\", \"Financials\"),\n",
        "    \"WEGE3\": (\"WEG S.A.\", \"Industrials\"),\n",
        "    \"ELET3\": (\"Eletrobras\", \"Utilities\"),\n",
        "    \"SUZB3\": (\"Suzano S.A.\", \"Materials\"),\n",
        "    \"MGLU3\": (\"Magazine Luiza S.A.\", \"Consumer Discretionary\"),\n",
        "    # EUA/ADR exemplos\n",
        "    \"PBR\": (\"Petróleo Brasileiro S.A. - Petrobras\", \"Energy\"),\n",
        "    \"AAPL\": (\"Apple Inc.\", \"Information Technology\"),\n",
        "    \"MSFT\": (\"Microsoft Corporation\", \"Information Technology\"),\n",
        "    \"GOOGL\": (\"Alphabet Inc.\", \"Communication Services\"),\n",
        "}\n",
        "\n",
        "\n",
        "# ---------------------- yfinance: coleta de nome e setor ----------------------\n",
        "def _get_profile_yfinance(symbol: str) -> Optional[Dict[str, str]]:\n",
        "    if yf is None:\n",
        "        return None\n",
        "    try:\n",
        "        t = yf.Ticker(symbol)\n",
        "        # .get_info() evita alguns avisos das versões recentes\n",
        "        try:\n",
        "            info = t.get_info()\n",
        "        except Exception:\n",
        "            info = getattr(t, \"info\", {}) or {}\n",
        "        if not info:\n",
        "            return None\n",
        "\n",
        "        # nome preferencial\n",
        "        name = info.get(\"longName\") or info.get(\"shortName\") or info.get(\"displayName\")\n",
        "        sector_raw = info.get(\"sector\") or info.get(\"industry\")  # às vezes só vem 'industry'\n",
        "        sector = _normalize_gics(sector_raw) or _normalize_gics(info.get(\"industry\"))  # segunda chance\n",
        "\n",
        "        if not name and not sector:\n",
        "            return None\n",
        "        return {\"name\": name, \"sector\": sector or sector_raw}\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# ---------------------- Normalização de datas ----------------------\n",
        "def _to_iso_date_str(d: str) -> str:\n",
        "    \"\"\"\n",
        "    Aceita 'YYYY-MM-DD', 'YYYY/MM/DD', 'DD/MM/YYYY', 'YYYY.MM.DD' etc. Retorna 'YYYY-MM-DD'.\n",
        "    \"\"\"\n",
        "    ds = (d or \"\").strip()\n",
        "    # tenta parsing direto\n",
        "    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%d/%m/%Y\", \"%Y.%m.%d\", \"%d-%m-%Y\", \"%m/%d/%Y\"):\n",
        "        try:\n",
        "            return datetime.strptime(ds, fmt).date().isoformat()\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback: tenta parser genérico do datetime.fromisoformat\n",
        "    try:\n",
        "        return datetime.fromisoformat(ds).date().isoformat()\n",
        "    except Exception:\n",
        "        # se tudo falhar, devolve a string original (melhor que quebrar)\n",
        "        return ds\n",
        "\n",
        "\n",
        "# ---------------------- Função pública ----------------------\n",
        "def montar_json_empresa(codigo_acao: str, final_do_teste: str, hoje_em_dia: str, *, debug: bool=False) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Retorna um dicionário no formato desejado:\n",
        "    {\n",
        "        \"codigo_da_ação\": \"<código original>\",\n",
        "        \"Nome_da_empresa\": \"<nome>\",\n",
        "        \"Setor_da_empresa\": \"<GICS>\",\n",
        "        \"Final_do_teste\": \"<YYYY-MM-DD>\",\n",
        "        \"hojé_em_dia\": \"<YYYY-MM-DD>\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    code_in = (codigo_acao or \"\").strip().upper()\n",
        "    # 1) tenta yfinance com variantes\n",
        "    name: Optional[str] = None\n",
        "    sector: Optional[str] = None\n",
        "\n",
        "    for sym in _guess_yahoo_variants(code_in):\n",
        "        prof = _get_profile_yfinance(sym)\n",
        "        if prof:\n",
        "            if debug:\n",
        "                print(f\"[DEBUG] yfinance hit: {sym} -> {prof}\")\n",
        "            name = name or prof.get(\"name\")\n",
        "            sector = prof.get(\"sector\") or sector\n",
        "            # se já fechou nome e setor GICS normalizado, para\n",
        "            if name and _normalize_gics(sector):\n",
        "                sector = _normalize_gics(sector)\n",
        "                break\n",
        "\n",
        "    # 2) fallback local (se ainda faltou algo)\n",
        "    if (not name or not sector) and code_in in _FALLBACK_PROFILE:\n",
        "        fb_name, fb_sector = _FALLBACK_PROFILE[code_in]\n",
        "        if debug:\n",
        "            print(f\"[DEBUG] fallback local usado para {code_in}\")\n",
        "        name = name or fb_name\n",
        "        sector = sector or fb_sector\n",
        "\n",
        "    # 3) normaliza setor para GICS (se possível)\n",
        "    sector_norm = _normalize_gics(sector) or sector or \"Unknown\"\n",
        "\n",
        "    # 4) normaliza datas\n",
        "    d_final = _to_iso_date_str(final_do_teste)\n",
        "    d_hoje = _to_iso_date_str(hoje_em_dia)\n",
        "\n",
        "    # formatação simples do nome (opcional)\n",
        "    def _format_name(n: Optional[str]) -> str:\n",
        "        if not n:\n",
        "            return code_in  # último recurso\n",
        "        # exemplos: \"Petróleo Brasileiro S.A. - Petrobras\" -> \"Petrobras - Petróleo Brasileiro S.A.\"\n",
        "        lower = n.lower()\n",
        "        if \"petrobras\" in lower and \"petróleo brasileiro\" in lower and \" - \" in n:\n",
        "            parts = [p.strip() for p in n.split(\" - \")]\n",
        "            if len(parts) == 2:\n",
        "                # inverte para priorizar \"Petrobras - ...\"\n",
        "                if \"petrobras\" in parts[1].lower():\n",
        "                    return f\"{parts[1]} - {parts[0]}\"\n",
        "        return n\n",
        "\n",
        "    nome_fmt = _format_name(name)\n",
        "\n",
        "    out = {\n",
        "        \"codigo_da_ação\": code_in,\n",
        "        \"Nome_da_empresa\": nome_fmt,\n",
        "        \"Setor_da_empresa\": sector_norm,\n",
        "        \"Final_do_teste\": d_final,\n",
        "        \"hojé_em_dia\": d_hoje,\n",
        "    }\n",
        "    if debug:\n",
        "        print(f\"[DEBUG] JSON final: {out}\")\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------- Exemplo rápido ----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    exemplo = montar_json_empresa(\"PETR3\", \"2025-06-30\", \"2025-09-06\", debug=True)\n",
        "    print(exemplo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvCjTQv1gZrm",
        "outputId": "ca654219-ba21-49e5-b2ec-97729e60afb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] yfinance hit: MSFT34.SA -> {'name': 'Microsoft Corporation', 'sector': 'Information Technology'}\n",
            "[DEBUG] JSON final: {'codigo_da_ação': 'MSFT34', 'Nome_da_empresa': 'Microsoft Corporation', 'Setor_da_empresa': 'Information Technology', 'Final_do_teste': '2025-06-30', 'hojé_em_dia': '2025-09-06'}\n",
            "{'codigo_da_ação': 'MSFT34', 'Nome_da_empresa': 'Microsoft Corporation', 'Setor_da_empresa': 'Information Technology', 'Final_do_teste': '2025-06-30', 'hojé_em_dia': '2025-09-06'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw feedparser trafilatura instructor openai httpx\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve secrets from Colab's Secrets Manager\n",
        "REDDIT_CLIENT_ID     = userdata.get(\"REDDIT_CLIENT_ID\")\n",
        "REDDIT_CLIENT_SECRET = userdata.get(\"REDDIT_CLIENT_SECRET\")\n",
        "REDDIT_USER_AGENT    = \"script:pesquisa-acao:v1.0 (by u/HugoHickman)\"\n",
        "OPENROUTER_API_KEY   = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# You can optionally print the values to confirm they are loaded (for debugging)\n",
        "# print(f\"REDDIT_CLIENT_ID: {REDDIT_CLIENT_ID}\")\n",
        "# print(f\"REDDIT_CLIENT_SECRET: {REDDIT_CLIENT_SECRET}\")\n",
        "# print(f\"REDDIT_USER_AGENT: {REDDIT_USER_AGENT}\")\n",
        "# print(f\"OPENROUTER_API_KEY: {OPENROUTER_API_KEY}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csqd9WCenkGD",
        "outputId": "a43fb798-c9a1-4511-fc39-1958b451606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.12/dist-packages (7.8.1)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.12/dist-packages (6.0.11)\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
            "Requirement already satisfied: instructor in /usr/local/lib/python3.12/dist-packages (1.11.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.104.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (0.28.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.12/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.12/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2025.8.3)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.4.3)\n",
            "Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (1.3.2)\n",
            "Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (1.9.3)\n",
            "Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.0.2)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (5.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from instructor) (3.12.15)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from instructor) (5.6.3)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor) (0.17.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from instructor) (3.1.6)\n",
            "Requirement already satisfied: jiter<0.11,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from instructor) (0.10.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (2.33.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (2.11.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from instructor) (2.32.4)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.7.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from instructor) (8.5.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (0.17.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.20.1)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (0.13.1)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.7.0->instructor) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.7.0->instructor) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.12/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p7MhQMyZMF3",
        "outputId": "5e7e4cd9-3652-4672-c0f2-96920040b6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-691633801.py:251: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  @validator(\"subreddits\", pre=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Rodando pesquisa_ação com DEBUG ... (threads=10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
            "ERROR:trafilatura.core:empty HTML tree: None\n",
            "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
            "ERROR:trafilatura.core:empty HTML tree: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Concluído em 276.88s.\n",
            "{\n",
            "  \"informacoes_da_empresa\": {\n",
            "    \"codigo_da_acao\": \"PETR3\",\n",
            "    \"nome_da_empresa\": \"Petrobras - Petróleo Brasileiro S.A.\"\n",
            "  },\n",
            "  \"analise_de_sentimentos\": {\n",
            "    \"hoje_em_dia\": {\n",
            "      \"data_de_referencia\": \"2025-09-06\",\n",
            "      \"Relatorios\": {\n",
            "        \"Empresa\": {\n",
            "          \"resumo\": \"A análise do ITR da Petrobras para o segundo trimestre de 2025, embora sem dados específicos disponíveis no momento, representa um ponto de avaliação crucial para o desempenho da empresa. Este relatório é fundamental para compreender a saúde financeira e as perspectivas operacionais da Petrobras no período. A ausência de informações detalhadas impede uma avaliação aprofundada de seus resultados.\",\n",
            "          \"nota\": 5\n",
            "        }\n",
            "      },\n",
            "      \"Noticias\": {\n",
            "        \"Empresa\": {\n",
            "          \"resumo\": \"A Petrobras tem enfrentado um período de alta volatilidade, com suas ações frequentemente impactadas pela queda nos preços do petróleo e pela perspectiva de aumento da produção da Opep+. A empresa registrou uma perda considerável de valor de mercado após a divulgação de seu balanço e está perdendo participação no mercado de gás. Apesar desses desafios, a demanda asiática tem impulsionado suas exportações, e a Petrobras busca captar US$2 bilhões em títulos globais, além de planejar o retorno ao mercado de etanol. Mudanças na governança corporativa e a estagnação do licenciamento ambiental na Foz do Amazonas também foram destaques.\",\n",
            "          \"nota\": 4\n",
            "        }\n",
            "      },\n",
            "      \"Reddit\": {\n",
            "        \"Empresa\": {\n",
            "          \"resumo\": \"Sem menções relevantes no Reddit no período.\",\n",
            "          \"nota\": 5\n",
            "        }\n",
            "      },\n",
            "      \"_debug\": {\n",
            "        \"ref_date\": \"2025-09-06\",\n",
            "        \"ref_cap\": \"2025-09-06\",\n",
            "        \"relatorios\": {\n",
            "          \"selected\": \"ITR PETROLEO BRASILEIRO S.A. PETROBRAS (2025-06-30)\",\n",
            "          \"date\": \"2025-06-30\",\n",
            "          \"n_candidates\": 7,\n",
            "          \"elapsed_s\": 79.36\n",
            "        },\n",
            "        \"llm\": {\n",
            "          \"provider\": \"gemini\",\n",
            "          \"model\": \"gemini-2.5-flash\",\n",
            "          \"fallback\": false\n",
            "        },\n",
            "        \"llm_expand\": {\n",
            "          \"model\": null,\n",
            "          \"fallback\": true,\n",
            "          \"error\": \"additionalProperties is not supported in the Gemini API.\",\n",
            "          \"cache\": false\n",
            "        },\n",
            "        \"noticias\": {\n",
            "          \"n_items\": 30,\n",
            "          \"elapsed_s\": 47.26,\n",
            "          \"queries\": [\n",
            "            \"PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\"\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") earnings\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") guidance\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") outage\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") lawsuit\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") strike\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") hack\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") recall\"\n",
            "          ]\n",
            "        },\n",
            "        \"reddit_query_meta\": {\n",
            "          \"queries\": [\n",
            "            \"PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\"\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") earnings\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") guidance\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") outage\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") lawsuit\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") strike\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") hack\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") recall\"\n",
            "          ],\n",
            "          \"subreddits\": [\n",
            "            \"Entrepreneur\",\n",
            "            \"Futurology\",\n",
            "            \"StockMarket\",\n",
            "            \"business\",\n",
            "            \"economy\",\n",
            "            \"energy\",\n",
            "            \"enewableenergy\",\n",
            "            \"financialindependence\",\n",
            "            \"investing\",\n",
            "            \"naturalgas\",\n",
            "            \"news\",\n",
            "            \"oil\",\n",
            "            \"options\",\n",
            "            \"renewableenergy\",\n",
            "            \"securityanalysis\",\n",
            "            \"stocks\",\n",
            "            \"technology\",\n",
            "            \"utilities\",\n",
            "            \"valueinvesting\",\n",
            "            \"wallstreetbets\"\n",
            "          ]\n",
            "        },\n",
            "        \"reddit\": {\n",
            "          \"n_items\": 0\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    \"Final_do_teste\": {\n",
            "      \"data_de_referencia\": \"2025-06-30\",\n",
            "      \"Relatorios\": {\n",
            "        \"Empresa\": {\n",
            "          \"resumo\": \"A análise do ITR da Petrobras para o segundo trimestre de 2025, embora sem dados específicos disponíveis no momento, representa um ponto de avaliação crucial para o desempenho da empresa. Este relatório é fundamental para compreender a saúde financeira e as perspectivas operacionais da Petrobras no período. A ausência de informações detalhadas impede uma avaliação aprofundada de seus resultados.\",\n",
            "          \"nota\": 5\n",
            "        }\n",
            "      },\n",
            "      \"Noticias\": {\n",
            "        \"Empresa\": {\n",
            "          \"resumo\": \"A Petrobras demonstrou forte desempenho no mercado, liderando os ganhos do Ibovespa em dias recentes e mostrando resiliência. A empresa também obteve sucesso em leilões de petróleo, garantindo lotes significativos. Além disso, há a perspectiva de uma parceria estratégica no setor de etanol, embora a recente redução nos preços da gasolina para distribuidoras possa impactar as receitas.\",\n",
            "          \"nota\": 8\n",
            "        }\n",
            "      },\n",
            "      \"Reddit\": {\n",
            "        \"Empresa\": {\n",
            "          \"resumo\": \"Sem menções relevantes no Reddit no período.\",\n",
            "          \"nota\": 5\n",
            "        }\n",
            "      },\n",
            "      \"_debug\": {\n",
            "        \"ref_date\": \"2025-06-30\",\n",
            "        \"ref_cap\": \"2025-06-30\",\n",
            "        \"relatorios\": {\n",
            "          \"selected\": \"ITR PETROLEO BRASILEIRO S.A. PETROBRAS (2025-06-30)\",\n",
            "          \"date\": \"2025-06-30\",\n",
            "          \"n_candidates\": 7,\n",
            "          \"elapsed_s\": 70.22\n",
            "        },\n",
            "        \"llm\": {\n",
            "          \"provider\": \"gemini\",\n",
            "          \"model\": \"gemini-2.5-flash\",\n",
            "          \"fallback\": false\n",
            "        },\n",
            "        \"llm_expand\": {\n",
            "          \"model\": null,\n",
            "          \"fallback\": true,\n",
            "          \"error\": \"additionalProperties is not supported in the Gemini API.\",\n",
            "          \"cache\": false\n",
            "        },\n",
            "        \"noticias\": {\n",
            "          \"n_items\": 5,\n",
            "          \"elapsed_s\": 22.77,\n",
            "          \"queries\": [\n",
            "            \"PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\"\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") earnings\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") guidance\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") outage\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") lawsuit\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") strike\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") hack\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") recall\"\n",
            "          ]\n",
            "        },\n",
            "        \"reddit_query_meta\": {\n",
            "          \"queries\": [\n",
            "            \"PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\"\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") earnings\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") guidance\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") outage\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") lawsuit\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") strike\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") hack\",\n",
            "            \"(PETR3 OR \\\"Petrobras - Petróleo Brasileiro\\\" OR \\\"Petrobras - Petróleo Brasileiro S.A.\\\") recall\"\n",
            "          ],\n",
            "          \"subreddits\": [\n",
            "            \"Entrepreneur\",\n",
            "            \"Futurology\",\n",
            "            \"StockMarket\",\n",
            "            \"business\",\n",
            "            \"economy\",\n",
            "            \"energy\",\n",
            "            \"enewableenergy\",\n",
            "            \"financialindependence\",\n",
            "            \"investing\",\n",
            "            \"naturalgas\",\n",
            "            \"news\",\n",
            "            \"oil\",\n",
            "            \"options\",\n",
            "            \"renewableenergy\",\n",
            "            \"securityanalysis\",\n",
            "            \"stocks\",\n",
            "            \"technology\",\n",
            "            \"utilities\",\n",
            "            \"valueinvesting\",\n",
            "            \"wallstreetbets\"\n",
            "          ]\n",
            "        },\n",
            "        \"reddit\": {\n",
            "          \"n_items\": 0\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"_meta\": {\n",
            "    \"elapsed_s\": 276.88,\n",
            "    \"threads\": 10\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# pesquisa_acao.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Pipeline robusto (degraus/fallbacks) para:\n",
        "- Relatórios (CVM + SEC/EDGAR + fallback ADR com XBRL KPIs)\n",
        "- Notícias (Google News RSS multilíngue) com aliases e expansões via LLM\n",
        "- Reddit (PRAW -> Reddit RSS) **qualitativo e universal**, com expansão de aliases/subreddits via LLM\n",
        "- Sumarização via Google AI Studio (Gemini 2.5 Flash) com heurística de fallback\n",
        "- Multilíngue: pt-BR, en-US, zh-CN (ou o que vier no config)\n",
        "- Dedup, retries/backoff, cache opcional\n",
        "- Logs detalhados e _debug para depuração\n",
        "\n",
        "GRÁTIS — sem dependência de APIs pagas.\n",
        "\n",
        "Instalação mínima:\n",
        "  pip install google-genai praw feedparser trafilatura httpx pydantic\n",
        "\n",
        "Opcionais (recomendado p/ robustez e PDFs):\n",
        "  pip install tenacity requests-cache url-normalize pymupdf pdfplumber\n",
        "\n",
        "Credenciais (Colab ou variáveis de ambiente):\n",
        "  GOOGLE_API_KEY\n",
        "  REDDIT_CLIENT_ID / REDDIT_CLIENT_SECRET / REDDIT_USER_AGENT   # PRAW (grátis)\n",
        "\n",
        "Importante p/ SEC:\n",
        "  export USER_AGENT=\"seu-app/1.0 (contato: voce@dominio.com; site: https://seusite.com)\"\n",
        "\n",
        "Config externo (opcional):\n",
        "- Variável PESQ_CONFIG aponta para um JSON. Se não existir, defaults internos serão usados.\n",
        "Exemplo de config.json:\n",
        "{\n",
        "  \"langs_news\": [\n",
        "    {\"hl\":\"pt-BR\",\"gl\":\"BR\",\"ceid\":\"BR:pt-BR\"},\n",
        "    {\"hl\":\"en-US\",\"gl\":\"US\",\"ceid\":\"US:en\"},\n",
        "    {\"hl\":\"zh-CN\",\"gl\":\"CN\",\"ceid\":\"CN:zh-Hans\"}\n",
        "  ],\n",
        "  \"gemini_models\": [\"gemini-2.5-flash\",\"gemini-2.5-flash-lite\",\"gemini-2.5-pro\"],\n",
        "  \"default_subreddits\": [\"stocks\",\"investing\",\"StockMarket\",\"valueinvesting\",\"wallstreetbets\"],\n",
        "  \"sector_hints\": {\n",
        "    \"Energy\":[\"energy\",\"oil\",\"naturalgas\",\"renewableenergy\",\"utilities\"]\n",
        "  },\n",
        "  \"blocked_domains\": [\"facebook.com\",\"instagram.com\",\"x.com\"],\n",
        "  \"max_workers\": 32,\n",
        "  \"http_timeout\": 25,\n",
        "  \"max_txt_chars\": 2000,\n",
        "  \"news\": {\"max_per_lang\": 18, \"max_total_llm\": 30},\n",
        "  \"reddit\": {\"max_items\": 220, \"min_per_sr\": 2}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "import os, io, re, json, zipfile, urllib.parse, logging, csv, unicodedata, time, random, pathlib\n",
        "import datetime as dt\n",
        "from typing import List, Optional, Tuple, Dict, Any, Set\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# --------- Base libs ---------\n",
        "import httpx\n",
        "import feedparser\n",
        "from trafilatura import extract\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "# --------- LLM (Gemini) ---------\n",
        "from google import genai\n",
        "\n",
        "# --------- Colab secrets (opcional) ---------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "except Exception:\n",
        "    userdata = None\n",
        "\n",
        "# --------- Reddit PRAW (opcional) ---------\n",
        "try:\n",
        "    import praw  # type: ignore\n",
        "except Exception:\n",
        "    praw = None\n",
        "\n",
        "# --------- Opcionais (robustez) ---------\n",
        "try:\n",
        "    from tenacity import retry, stop_after_attempt, wait_exponential_jitter, retry_if_exception_type\n",
        "except Exception:\n",
        "    retry = None\n",
        "\n",
        "try:\n",
        "    from url_normalize import url_normalize\n",
        "except Exception:\n",
        "    def url_normalize(u: str) -> str:\n",
        "        return u\n",
        "\n",
        "# PDFs\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "except Exception:\n",
        "    fitz = None\n",
        "\n",
        "try:\n",
        "    import pdfplumber\n",
        "except Exception:\n",
        "    pdfplumber = None\n",
        "\n",
        "# Cache (requests-cache ajuda se você também usa requests em outros trechos)\n",
        "try:\n",
        "    import requests_cache\n",
        "except Exception:\n",
        "    requests_cache = None\n",
        "\n",
        "# ============= CONFIG / DEFAULTS =============\n",
        "def _load_config() -> Dict[str, Any]:\n",
        "    cfg_path = os.getenv(\"PESQ_CONFIG\")\n",
        "    if cfg_path and os.path.exists(cfg_path):\n",
        "        try:\n",
        "            with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback: procurar \"config.json\" no CWD\n",
        "    if os.path.exists(\"config.json\"):\n",
        "        try:\n",
        "            with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # default minimal\n",
        "    return {\n",
        "        \"langs_news\": [\n",
        "            {\"hl\": \"pt-BR\", \"gl\": \"BR\", \"ceid\": \"BR:pt-BR\"},\n",
        "            {\"hl\": \"en-US\", \"gl\": \"US\", \"ceid\": \"US:en\"},\n",
        "            {\"hl\": \"zh-CN\", \"gl\": \"CN\", \"ceid\": \"CN:zh-Hans\"},\n",
        "        ],\n",
        "        \"gemini_models\": [\"gemini-2.5-flash\",\"gemini-2.5-flash-lite\",\"gemini-2.5-pro\"],\n",
        "        \"default_subreddits\": [\n",
        "            \"stocks\",\"StockMarket\",\"wallstreetbets\",\"valueinvesting\",\"securityanalysis\",\n",
        "            \"investing\",\"financialindependence\",\"options\",\"economy\",\"technology\",\n",
        "            \"business\",\"Entrepreneur\",\"news\",\"Futurology\"\n",
        "        ],\n",
        "        \"sector_hints\": {\n",
        "            \"Technology\": [\"tech\",\"hardware\",\"software\",\"programming\",\"cybersecurity\"],\n",
        "            \"Energy\": [\"energy\",\"oil\",\"naturalgas\",\"renewableenergy\",\"utilities\"],\n",
        "            \"Financials\": [\"banking\",\"finance\",\"wallstreetbetsELITE\"],\n",
        "            \"Healthcare\": [\"biotech\",\"medicine\",\"healthcare\",\"pharmacy\"],\n",
        "            \"Consumer\": [\"retail\",\"ecommerce\",\"marketing\",\"advertising\"],\n",
        "            \"Industrial\": [\"manufacturing\",\"supplychain\",\"aviation\"],\n",
        "            \"Telecom\": [\"telecom\",\"5Gtechnology\"],\n",
        "            \"Materials\": [\"mining\",\"chemistry\"],\n",
        "            \"Utilities\": [\"utilities\"]\n",
        "        },\n",
        "        \"blocked_domains\": [\"facebook.com\",\"instagram.com\",\"x.com\"],\n",
        "        \"max_workers\": min(32, (os.cpu_count() or 4) * 5),\n",
        "        \"http_timeout\": 25,\n",
        "        \"max_txt_chars\": 2000,\n",
        "        \"news\": {\"max_per_lang\": 18, \"max_total_llm\": 30},\n",
        "        \"reddit\": {\"max_items\": 220, \"min_per_sr\": 2}\n",
        "    }\n",
        "\n",
        "CONFIG = _load_config()\n",
        "\n",
        "# ============= LOGGING =============\n",
        "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n",
        "logging.basicConfig(\n",
        "    level=LOG_LEVEL,\n",
        "    format=\"%(levelname)s:%(name)s:%(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(\"pesquisa_acao\")\n",
        "for _name in [\"trafilatura\", \"trafilatura.core\", \"trafilatura.downloads\"]:\n",
        "    logging.getLogger(_name).setLevel(logging.ERROR)\n",
        "\n",
        "# ============= PERFORMANCE / LIMITES =============\n",
        "MAX_WORKERS = int(os.getenv(\"THREADS\", str(CONFIG.get(\"max_workers\", 32))))\n",
        "HTTP_TIMEOUT = int(CONFIG.get(\"http_timeout\", 25))\n",
        "MAX_TXT_CHARS = int(CONFIG.get(\"max_txt_chars\", 2000))\n",
        "\n",
        "MAX_NEWS_PER_LANG = int(CONFIG.get(\"news\", {}).get(\"max_per_lang\", 18))\n",
        "MAX_NEWS_TOTAL_FOR_LLM = int(CONFIG.get(\"news\", {}).get(\"max_total_llm\", 30))\n",
        "\n",
        "MAX_REDDIT_ITEMS = int(CONFIG.get(\"reddit\", {}).get(\"max_items\", 220))\n",
        "REDDIT_FETCH_PER_SR_MIN = int(CONFIG.get(\"reddit\", {}).get(\"min_per_sr\", 2))\n",
        "\n",
        "# ============= CONFIG GERAL =============\n",
        "USER_AGENT = os.getenv(\"USER_AGENT\", \"pesquisa-acao/1.4 (contato: voce@example.com)\")\n",
        "LANGS_NEWS = CONFIG.get(\"langs_news\", [])\n",
        "GEMINI_MODEL_CANDIDATES = CONFIG.get(\"gemini_models\", [\"gemini-2.5-flash\",\"gemini-2.5-flash-lite\",\"gemini-2.5-pro\"])\n",
        "DEFAULT_SUBREDDITS = CONFIG.get(\"default_subreddits\", [])\n",
        "SECTOR_SUBREDDITS = CONFIG.get(\"sector_hints\", {})\n",
        "BLOCKED_DOMAINS = set(CONFIG.get(\"blocked_domains\", []))\n",
        "\n",
        "# ============= PEQUENOS SEEDS (mantidos mínimos) =============\n",
        "# BR → ADR: usado apenas como \"seed\"; LLM também sugere tickers ADR e o código testa via SEC.\n",
        "BR_TO_ADR_SEED = {\n",
        "    \"PETR\": [\"PBR\", \"PBR.A\"],\n",
        "    \"VALE\": [\"VALE\"],\n",
        "    \"BBDC\": [\"BBD\", \"BBDO\"],\n",
        "    \"ITUB\": [\"ITUB\"],\n",
        "    \"ABEV\": [\"ABEV\"],\n",
        "    \"ELET\": [\"EBR\", \"EBR.B\"],\n",
        "    \"SUZB\": [\"SUZ\"],\n",
        "    \"BBAS\": [\"BDORY\"],\n",
        "    \"WEGE\": [\"WEGEY\"],\n",
        "}\n",
        "\n",
        "# Classes comuns de tickers US (seed mínima; LLM cobre o resto)\n",
        "US_CLASS_TICKERS_SEED = {\n",
        "    \"GOOG\": [\"GOOG\", \"GOOGL\"],\n",
        "    \"GOOGL\": [\"GOOG\", \"GOOGL\"],\n",
        "    \"BRK.B\": [\"BRK.B\", \"BRK.A\"],\n",
        "    \"BRK.A\": [\"BRK.A\", \"BRK.B\"],\n",
        "    \"META\": [\"META\", \"FB\"],\n",
        "}\n",
        "\n",
        "# ============= DISK CACHE p/ expansões LLM (evita custo/latência) =============\n",
        "CACHE_DIR = pathlib.Path(os.getenv(\"PESQ_CACHE_DIR\", pathlib.Path.home() / \".cache\" / \"pesquisa_acao\"))\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPAND_CACHE_FILE = CACHE_DIR / \"expand_cache.json\"\n",
        "\n",
        "def _load_expand_cache() -> Dict[str, Any]:\n",
        "    if EXPAND_CACHE_FILE.exists():\n",
        "        try:\n",
        "            return json.load(open(EXPAND_CACHE_FILE, \"r\", encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def _save_expand_cache(cache: Dict[str, Any]) -> None:\n",
        "    try:\n",
        "        json.dump(cache, open(EXPAND_CACHE_FILE, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "EXPAND_CACHE = _load_expand_cache()\n",
        "\n",
        "# ============= SCHEMAS =============\n",
        "class Item(BaseModel):\n",
        "    title: str\n",
        "    url: str\n",
        "    published_at: Optional[str] = None\n",
        "    summary: Optional[str] = None\n",
        "    sentiment: Optional[float] = Field(default=None, ge=-1, le=1)\n",
        "\n",
        "class BatchLLM(BaseModel):\n",
        "    items: List[Item]\n",
        "    summary: str\n",
        "    note_score: int = Field(ge=0, le=100)\n",
        "    note_label: str\n",
        "    rationale: str\n",
        "\n",
        "class LLMExpand(BaseModel):\n",
        "    \"\"\"Resposta do LLM para aliases e subreddits.\"\"\"\n",
        "    aliases: List[str] = Field(default_factory=list, description=\"nomes alternativos, tickers, marcas, produtos\")\n",
        "    translations: Dict[str, List[str]] = Field(default_factory=dict, description=\"ex.: {'pt': [...], 'en':[...], 'zh':[...]} \")\n",
        "    subreddits: List[str] = Field(default_factory=list, description=\"subreddits relevantes (sem prefixo r/)\")\n",
        "    focus_terms: List[str] = Field(default_factory=list, description=\"termos qualitativos (earnings, lawsuit, strike, hack, ... até 25)\")\n",
        "\n",
        "    @validator(\"subreddits\", pre=True)\n",
        "    def _clean_sr(cls, v):\n",
        "        out = []\n",
        "        for s in v or []:\n",
        "            s = str(s).strip().lstrip(\"r/\").split(\"/\")[0]\n",
        "            if s:\n",
        "                out.append(s)\n",
        "        return out\n",
        "\n",
        "# ============= HELPERS =============\n",
        "def _now_utc_date() -> dt.date:\n",
        "    return dt.datetime.now(dt.timezone.utc).date()\n",
        "\n",
        "def _cap_ref_date(ref_date: dt.date) -> dt.date:\n",
        "    today = _now_utc_date()\n",
        "    if ref_date > today:\n",
        "        logger.info(f\"[Datas] ref_date {ref_date} > hoje {today}. Usando ref_cap = hoje.\")\n",
        "    return min(ref_date, today)\n",
        "\n",
        "def _to_date(s: str) -> dt.date:\n",
        "    s = s.strip()\n",
        "    m = re.match(r\"^\\s*(\\d{4})[-/](\\d{1,2})[-/](\\d{1,2})\\s*$\", s)\n",
        "    if m:\n",
        "        y, mm, dd = map(int, m.groups())\n",
        "        return dt.date(y, mm, dd)\n",
        "    if s.startswith(\"date(\") and s.endswith(\")\"):\n",
        "        nums = [int(x) for x in re.findall(r\"\\d+\", s)]\n",
        "        return dt.date(nums[0], nums[1], nums[2])\n",
        "    return dt.date.fromisoformat(s)\n",
        "\n",
        "def _parse_dt_iso(s: Optional[str]) -> Optional[dt.datetime]:\n",
        "    if not s: return None\n",
        "    ss = s.strip().replace(\"Z\", \"+00:00\")\n",
        "    try:\n",
        "        d = dt.datetime.fromisoformat(ss)\n",
        "        return d if d.tzinfo else d.replace(tzinfo=dt.timezone.utc)\n",
        "    except Exception:\n",
        "        try:\n",
        "            d = dt.date.fromisoformat(ss[:10])\n",
        "            return dt.datetime.combine(d, dt.time(0,0), tzinfo=dt.timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def _within_window(published_at: Optional[str], ref_date: dt.date, lookback_days: int) -> bool:\n",
        "    if not published_at: return False\n",
        "    t = _parse_dt_iso(published_at)\n",
        "    if not t:\n",
        "        try:\n",
        "            d = dt.date.fromisoformat(published_at[:10])\n",
        "            t = dt.datetime.combine(d, dt.time(0,0), tzinfo=dt.timezone.utc)\n",
        "        except Exception:\n",
        "            return False\n",
        "    d = t.date()\n",
        "    return (d <= ref_date) and (d >= ref_date - dt.timedelta(days=lookback_days))\n",
        "\n",
        "def _nota_0a10(score_0a100: int) -> int:\n",
        "    return max(0, min(10, round(score_0a100 / 10)))\n",
        "\n",
        "def _normalize_name(s: Optional[str]) -> str:\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = unicodedata.normalize(\"NFKD\", s)\n",
        "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
        "    s = re.sub(r\"[^A-Za-z0-9]+\", \" \", s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s).upper()\n",
        "\n",
        "def _us_ticker_variants(ticker: str) -> List[str]:\n",
        "    up = ticker.upper()\n",
        "    if up in US_CLASS_TICKERS_SEED:\n",
        "        return US_CLASS_TICKERS_SEED[up][:]\n",
        "    m = re.match(r\"^([A-Z]{1,4})[.\\-]([A-Z])$\", up)\n",
        "    if m:\n",
        "        base, cls = m.groups()\n",
        "        other = \"A\" if cls == \"B\" else \"B\"\n",
        "        return [up, f\"{base}.{other}\"]\n",
        "    return [up]\n",
        "\n",
        "def _name_matches(norm_row_name: str, aliases: List[str]) -> bool:\n",
        "    for alias in aliases:\n",
        "        if alias and (alias in norm_row_name or norm_row_name in alias):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _get_secret(name: str) -> Optional[str]:\n",
        "    val = None\n",
        "    if userdata is not None:\n",
        "        try:\n",
        "            val = userdata.get(name)\n",
        "        except Exception:\n",
        "            val = None\n",
        "    return val or os.getenv(name)\n",
        "\n",
        "def _hostname(url: str) -> str:\n",
        "    try:\n",
        "        return urllib.parse.urlparse(url).hostname or \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ============= LEXICON / HEURÍSTICA =============\n",
        "def _lexicon() -> Dict[str, set]:\n",
        "    pos = {\n",
        "        \"alta\",\"subida\",\"otimista\",\"positivo\",\"crescimento\",\"melhora\",\"recorde\",\"ganho\",\"lucro\",\n",
        "        \"aumenta\",\"acima\",\"supera\",\"favorável\",\"forte\",\"fortes\",\"expansão\",\"upgrade\",\"compra\",\n",
        "        \"up\",\"beat\",\"growth\",\"improves\",\"record\",\"profit\",\"expansion\",\"upgrade\",\"买入\",\"增长\",\"改善\"\n",
        "    }\n",
        "    neg = {\n",
        "        \"queda\",\"cai\",\"negativo\",\"piora\",\"prejuízo\",\"risco\",\"problema\",\"baixa\",\"reduz\",\"abaixo\",\n",
        "        \"perde\",\"alerta\",\"fraqueza\",\"fraude\",\"downgrade\",\"venda\",\"investigação\",\n",
        "        \"down\",\"miss\",\"decline\",\"loss\",\"risk\",\"问题\",\"下跌\",\"亏损\",\"降级\",\"卖出\"\n",
        "    }\n",
        "    return {\"pos\": pos, \"neg\": neg}\n",
        "\n",
        "def _heuristic_sentiment(items: List[Item]) -> Tuple[List[Item], str, int]:\n",
        "    lex = _lexicon()\n",
        "    pos, neg = lex[\"pos\"], lex[\"neg\"]\n",
        "    scores = []\n",
        "    for it in items:\n",
        "        text = f\"{it.title} {it.summary or ''}\".lower()\n",
        "        p = sum(1 for w in pos if w in text)\n",
        "        n = sum(1 for w in neg if w in text)\n",
        "        sc = 0.0\n",
        "        if p or n:\n",
        "            sc = (p - n) / max(1, (p + n))\n",
        "        it.sentiment = sc\n",
        "        if not it.summary:\n",
        "            it.summary = it.title\n",
        "        scores.append(sc)\n",
        "    agg = sum(scores)/len(scores) if scores else 0.0\n",
        "    note100 = int(round((agg + 1.0) * 50))\n",
        "    note10  = _nota_0a10(note100)\n",
        "    label = \"neutral\"\n",
        "    if   agg >= 0.75: label = \"strong_buy\"\n",
        "    elif agg >= 0.50: label = \"buy\"\n",
        "    elif agg <= -0.75: label = \"strong_sell\"\n",
        "    elif agg <= -0.50: label = \"sell\"\n",
        "    resumo = f\"Heuristic sentiment (pt/en/zh): mean={agg:.2f} → {note10}/10 ({label}).\"\n",
        "    return items, resumo, note10\n",
        "\n",
        "# ============= GEMINI (Google AI Studio) =============\n",
        "def _genai_client(api_key: Optional[str] = None) -> genai.Client:\n",
        "    api_key = api_key or _get_secret(\"GOOGLE_API_KEY\") or _get_secret(\"GEMINI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Defina GOOGLE_API_KEY (ou GEMINI_API_KEY). https://aistudio.google.com/app/apikey\")\n",
        "    return genai.Client(api_key=api_key)\n",
        "\n",
        "def _normalize_items_for_llm(items: List[Item], max_items: int = 18) -> List[Item]:\n",
        "    seen = set(); out = []\n",
        "    for it in items:\n",
        "        key = (url_normalize(it.url) if it.url else \"\").strip().lower() or it.title.strip().lower()\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        if it.summary and len(it.summary) > MAX_TXT_CHARS:\n",
        "            it.summary = it.summary[:MAX_TXT_CHARS]\n",
        "        out.append(it)\n",
        "        if len(out) >= max_items:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "def _genai_resumir_e_notar(items: List[Item], empresa: str, ticker: str, setor: str, fonte: str,\n",
        "                            debug: Dict[str, Any], kpis: Optional[Dict[str, Tuple[str,float,str]]] = None) -> Tuple[List[Item], str, int]:\n",
        "    items = _normalize_items_for_llm(items, max_items=18)\n",
        "    compact = \"\\n\".join([f\"- {it.title}\\n{(it.summary or '')}\" for it in items])\n",
        "\n",
        "    kpi_txt = \"\"\n",
        "    if kpis:\n",
        "        parts = []\n",
        "        for k,(date,val,unit) in kpis.items():\n",
        "            parts.append(f\"{k}={val} {unit} ({date})\")\n",
        "        kpi_txt = \"KPIs XBRL até a data: \" + \"; \".join(parts) + \".\\n\"\n",
        "\n",
        "    prompt = (\n",
        "        \"Você é analista de equity. Responda em **português do Brasil**.\\n\"\n",
        "        \"Fontes podem estar em PT/EN/ZH; normalize as conclusões.\\n\"\n",
        "        f\"Empresa: {empresa} ({ticker}) | Setor: {setor} | Fonte: {fonte}\\n\"\n",
        "        + (kpi_txt if kpi_txt else \"\") +\n",
        "        \"Para cada item, gere UMA frase de síntese e um sentimento em [-1,1]. \"\n",
        "        \"Depois gere: (1) resumo agregado (2–4 frases) (2) nota de 0 a 100 e \"\n",
        "        \"label (strong_buy|buy|neutral|sell|strong_sell) apenas com base nestes itens e nos KPIs (se houver).\\n\"\n",
        "        f\"ITENS:\\n{compact}\"\n",
        "    )\n",
        "\n",
        "    last_error = None\n",
        "    for model in GEMINI_MODEL_CANDIDATES:\n",
        "        try:\n",
        "            logger.info(f\"[Gemini] Tentando modelo: {model}\")\n",
        "            client = _genai_client()\n",
        "            response = client.models.generate_content(\n",
        "                model=model,\n",
        "                contents=prompt,\n",
        "                config={\n",
        "                    \"temperature\": 0,\n",
        "                    \"response_mime_type\": \"application/json\",\n",
        "                    \"response_schema\": BatchLLM,\n",
        "                },\n",
        "            )\n",
        "            parsed = getattr(response, \"parsed\", None)\n",
        "            if parsed is None:\n",
        "                text = (response.text or \"\").strip()\n",
        "                parsed = BatchLLM(**(json.loads(text) if text else {}))\n",
        "            nota = _nota_0a10(parsed.note_score)\n",
        "            out_items: List[Item] = []\n",
        "            for i, it in enumerate(items):\n",
        "                if i < len(parsed.items):\n",
        "                    it.summary   = parsed.items[i].summary or it.summary\n",
        "                    it.sentiment = parsed.items[i].sentiment\n",
        "                out_items.append(it)\n",
        "            debug[\"llm\"] = {\"provider\": \"gemini\", \"model\": model, \"fallback\": False}\n",
        "            return out_items, parsed.summary, nota\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            logger.warning(f\"[Gemini] Falhou com {model}: {type(e).__name__} — tentando próximo.\")\n",
        "    logger.warning(f\"[Gemini] Todas as tentativas falharam: {type(last_error).__name__ if last_error else 'Erro desconhecido'}. Usando heurística.\")\n",
        "    debug[\"llm\"] = {\"provider\": \"gemini\", \"model\": None, \"fallback\": True, \"error\": str(last_error)}\n",
        "    return _heuristic_sentiment(items)\n",
        "\n",
        "# ---------- LLM para expansão de aliases e subreddits ----------\n",
        "def _expand_cache_key(empresa: str, ticker: str, setor: str) -> str:\n",
        "    return f\"{ticker.upper()}||{empresa.strip()}||{setor.strip()}\"\n",
        "\n",
        "def _llm_expand_aliases_and_subreddits(empresa: str, ticker: str, setor: str, debug: Dict[str,Any]) -> LLMExpand:\n",
        "    \"\"\"Usa Gemini para gerar aliases, traduções, subreddits e termos. Cacheia em disco (TTL simples 7 dias).\"\"\"\n",
        "    key = _expand_cache_key(empresa, ticker, setor)\n",
        "    rec = EXPAND_CACHE.get(key)\n",
        "    if rec and isinstance(rec, dict):\n",
        "        try:\n",
        "            ts = float(rec.get(\"_ts\", 0))\n",
        "            if time.time() - ts < 7*24*3600:\n",
        "                parsed = LLMExpand(**rec.get(\"payload\", {}))\n",
        "                debug[\"llm_expand\"] = {\"model\": rec.get(\"model\",\"cache\"), \"fallback\": rec.get(\"fallback\", False), \"cache\": True}\n",
        "                return parsed\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    prompt = (\n",
        "        \"Atue como pesquisador de equity. Gere um JSON compacto com campos:\\n\"\n",
        "        \"aliases: [strings]\\n\"\n",
        "        \"translations: { 'pt': [..], 'en':[...], 'zh':[...]} (se aplicável)\\n\"\n",
        "        \"subreddits: [strings]  # sem prefixo r/\\n\"\n",
        "        \"focus_terms: [strings] # até 25 termos qualitativos úteis em buscas (earnings, guidance, outage, lawsuit, strike, hack, recall, boycott, layoff, churn, price hike, product issue, CEO, CFO, union, subsidy, ban, regulation, sanction, antitrust, class action, short seller, whistleblower, supply chain, shutdown, plant, outage, cybersecurity, data breach, downtime, margin, debt, downgrade, upgrade)\\n\\n\"\n",
        "        \"Regras:\\n- Limite aliases a 20, subreddits a 20.\\n- Inclua tickers-irmãos (ex.: GOOG/GOOGL; BRK.A/BRK.B) se existir.\\n- Para empresa estrangeira com ADR, inclua tickers ADR se conhecidos.\\n- Só liste subreddits que existem publicamente e façam sentido.\\n- Evite duplicatas; normalize subreddits sem 'r/'.\\n\"\n",
        "        f\"Empresa: {empresa}\\nTicker: {ticker}\\nSetor: {setor}\\n\"\n",
        "    )\n",
        "    last_error = None\n",
        "    for model in GEMINI_MODEL_CANDIDATES:\n",
        "        try:\n",
        "            client = _genai_client()\n",
        "            response = client.models.generate_content(\n",
        "                model=model,\n",
        "                contents=prompt,\n",
        "                config={\n",
        "                    \"temperature\": 0.2,\n",
        "                    \"response_mime_type\": \"application/json\",\n",
        "                    \"response_schema\": LLMExpand,\n",
        "                },\n",
        "            )\n",
        "            parsed = getattr(response, \"parsed\", None)\n",
        "            if parsed is None:\n",
        "                text = (response.text or \"\").strip()\n",
        "                parsed = LLMExpand(**(json.loads(text) if text else {}))\n",
        "            debug[\"llm_expand\"] = {\"model\": model, \"fallback\": False, \"cache\": False}\n",
        "            # save cache\n",
        "            EXPAND_CACHE[key] = {\"_ts\": time.time(), \"model\": model, \"fallback\": False, \"payload\": parsed.dict()}\n",
        "            _save_expand_cache(EXPAND_CACHE)\n",
        "            return parsed\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            logger.debug(f\"[LLM expand] falhou {model}: {e}\")\n",
        "\n",
        "    # Fallback heurístico\n",
        "    aliases: Set[str] = set([empresa.strip(), ticker.strip()] + _us_ticker_variants(ticker))\n",
        "    corp = [\" Inc\", \" Inc.\", \" Corporation\", \" Corp\", \" Corp.\", \" Ltd\", \" Ltd.\", \" S.A.\", \" S A\", \" PLC\", \" N.V.\", \", SA\", \", Inc.\"]\n",
        "    for c in corp:\n",
        "        if empresa.endswith(c):\n",
        "            aliases.add(empresa.replace(c, \"\").strip())\n",
        "    base_sr = set(DEFAULT_SUBREDDITS)\n",
        "    base_sr.update(SECTOR_SUBREDDITS.get(setor, []))\n",
        "    focus_terms = [\n",
        "        \"earnings\",\"guidance\",\"outage\",\"lawsuit\",\"strike\",\"hack\",\"recall\",\"boycott\",\"layoff\",\n",
        "        \"churn\",\"price hike\",\"product issue\",\"CEO\",\"CFO\",\"union\",\"subsidy\",\"ban\",\"regulation\",\n",
        "        \"sanction\",\"antitrust\",\"class action\",\"short seller\",\"whistleblower\",\"supply chain\",\"shutdown\",\n",
        "        \"plant\",\"cybersecurity\",\"data breach\",\"downtime\",\"margin\",\"debt\",\"downgrade\",\"upgrade\"\n",
        "    ]\n",
        "    parsed = LLMExpand(\n",
        "        aliases=list({a for a in aliases if a})[:20],\n",
        "        translations={},\n",
        "        subreddits=[s for s in base_sr][:20],\n",
        "        focus_terms=focus_terms[:25]\n",
        "    )\n",
        "    debug[\"llm_expand\"] = {\"model\": None, \"fallback\": True, \"error\": str(last_error), \"cache\": False}\n",
        "    return parsed\n",
        "\n",
        "# ============= Cache/retry HTTP ============\n",
        "if requests_cache and os.getenv(\"ENABLE_REQUESTS_CACHE\", \"0\") == \"1\":\n",
        "    try:\n",
        "        requests_cache.install_cache(\"pesquisa_acao_cache\", backend=\"sqlite\", expire_after=6*3600)\n",
        "        logger.info(\"[Cache] requests-cache habilitado (6h).\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _retryable():\n",
        "    if retry:\n",
        "        return retry(\n",
        "            reraise=True,\n",
        "            stop=stop_after_attempt(3),\n",
        "            wait=wait_exponential_jitter(initial=1, max=8),\n",
        "            retry=retry_if_exception_type(Exception),\n",
        "        )\n",
        "    def deco(fn): return fn\n",
        "    return deco\n",
        "\n",
        "@_retryable()\n",
        "def _http_get(url: str, timeout: int = HTTP_TIMEOUT) -> Optional[str]:\n",
        "    if any(d in (url.lower()) for d in BLOCKED_DOMAINS):\n",
        "        return None\n",
        "    headers = {\"User-Agent\": USER_AGENT, \"Accept\": \"text/html,application/xhtml+xml\"}\n",
        "    with httpx.Client(headers=headers, timeout=timeout, follow_redirects=True) as cli:\n",
        "        r = cli.get(url)\n",
        "        if r.status_code != 200 or not r.text:\n",
        "            raise RuntimeError(f\"GET {url} -> {r.status_code}\")\n",
        "        time.sleep(0.07 + random.random()*0.08)  # rate-limit leve\n",
        "        return r.text\n",
        "\n",
        "def _fetch_clean_text(url: str) -> str:\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    try:\n",
        "        html = _http_get(url)\n",
        "    except Exception:\n",
        "        html = None\n",
        "    if not html:\n",
        "        return \"\"\n",
        "    try:\n",
        "        txt = extract(html, include_links=False) or \"\"\n",
        "    except Exception:\n",
        "        txt = \"\"\n",
        "    return txt.strip()\n",
        "\n",
        "def _fetch_pdf_text(url: str, limit_chars: int = 10000) -> str:\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        if fitz:\n",
        "            with httpx.Client(headers={\"User-Agent\": USER_AGENT}, timeout=HTTP_TIMEOUT) as cli:\n",
        "                r = cli.get(url)\n",
        "                if r.status_code == 200:\n",
        "                    with fitz.open(stream=r.content, filetype=\"pdf\") as doc:\n",
        "                        for page in doc:\n",
        "                            text += page.get_text()\n",
        "                            if len(text) >= limit_chars:\n",
        "                                break\n",
        "                    return text[:limit_chars]\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        if pdfplumber:\n",
        "            with httpx.Client(headers={\"User-Agent\": USER_AGENT}, timeout=HTTP_TIMEOUT) as cli:\n",
        "                r = cli.get(url)\n",
        "                if r.status_code == 200:\n",
        "                    with io.BytesIO(r.content) as f:\n",
        "                        with pdfplumber.open(f) as doc:\n",
        "                            for page in doc.pages:\n",
        "                                text += page.extract_text() or \"\"\n",
        "                                if len(text) >= limit_chars:\n",
        "                                    break\n",
        "                    return text[:limit_chars]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "def _extract_text_general(url: str) -> str:\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    ul = url.lower()\n",
        "    if ul.endswith(\".pdf\") and (fitz or pdfplumber):\n",
        "        return _fetch_pdf_text(url, limit_chars=MAX_TXT_CHARS)\n",
        "    return _fetch_clean_text(url)[:MAX_TXT_CHARS]\n",
        "\n",
        "# ============= NOTÍCIAS (Google News RSS) =============\n",
        "def _google_news_rss(query: str, lang: Dict[str,str]) -> str:\n",
        "    q = urllib.parse.quote_plus(query)\n",
        "    return f\"https://news.google.com/rss/search?q={q}&hl={lang['hl']}&gl={lang['gl']}&ceid={lang['ceid']}\"\n",
        "\n",
        "def _dedup(items: List[Item]) -> List[Item]:\n",
        "    seen = set(); out = []\n",
        "    for it in items:\n",
        "        key = (url_normalize(it.url) if it.url else \"\").strip().lower() or it.title.strip().lower()\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append(it)\n",
        "    return out\n",
        "\n",
        "def _enrich_bodies_parallel(items: List[Item]) -> None:\n",
        "    def _worker(it: Item) -> str:\n",
        "        return _extract_text_general(it.url)\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = {ex.submit(_worker, it): it for it in items}\n",
        "        for fut in as_completed(futs):\n",
        "            it = futs[fut]\n",
        "            try:\n",
        "                body = fut.result() or \"\"\n",
        "            except Exception:\n",
        "                body = \"\"\n",
        "            if body:\n",
        "                it.summary = body\n",
        "\n",
        "def _build_news_queries(empresa: str, ticker: str, setor: str, expand: LLMExpand) -> List[str]:\n",
        "    aliases = set([empresa, ticker] + _us_ticker_variants(ticker))\n",
        "    aliases.update(expand.aliases or [])\n",
        "    for lang, arr in (expand.translations or {}).items():\n",
        "        aliases.update(arr or [])\n",
        "    aliases = {a.strip() for a in aliases if a and len(a.strip()) >= 2}\n",
        "    focus = (expand.focus_terms or [])[:10]\n",
        "    queries = []\n",
        "    big_or = \" OR \".join([f\"\\\"{a}\\\"\" if \" \" in a else a for a in sorted(aliases)])\n",
        "    queries.append(big_or)\n",
        "    for t in focus:\n",
        "        queries.append(f\"({big_or}) {t}\")\n",
        "    return queries[:8]\n",
        "\n",
        "def _collect_news(company: str, ticker: str, setor: str, expand: LLMExpand,\n",
        "                  ref_cap: dt.date, lookback_days: int) -> List[Item]:\n",
        "    items: List[Item] = []\n",
        "    queries = _build_news_queries(company, ticker, setor, expand)\n",
        "    langs = LANGS_NEWS or [\n",
        "        {\"hl\": \"pt-BR\", \"gl\": \"BR\", \"ceid\": \"BR:pt-BR\"},\n",
        "        {\"hl\": \"en-US\", \"gl\": \"US\", \"ceid\": \"US:en\"},\n",
        "        {\"hl\": \"zh-CN\", \"gl\": \"CN\", \"ceid\": \"CN:zh-Hans\"},\n",
        "    ]\n",
        "\n",
        "    for L in langs:\n",
        "        lang_items: List[Item] = []\n",
        "        for q in queries:\n",
        "            feed_url = _google_news_rss(q, L)\n",
        "            try:\n",
        "                feed = feedparser.parse(feed_url)\n",
        "            except Exception:\n",
        "                continue\n",
        "            for e in feed.entries[:MAX_NEWS_PER_LANG]:\n",
        "                link = e.get(\"link\") or e.get(\"id\") or \"\"\n",
        "                title = e.get(\"title\") or \"\"\n",
        "                pub = None\n",
        "                if getattr(e, \"published_parsed\", None):\n",
        "                    pub = dt.datetime(*e.published_parsed[:6], tzinfo=dt.timezone.utc).isoformat()\n",
        "                it = Item(title=title, url=link, published_at=pub)\n",
        "                if _within_window(it.published_at, ref_cap, lookback_days):\n",
        "                    lang_items.append(it)\n",
        "        logger.info(f\"[Noticias] {L.get('hl','?')}: {len(lang_items)} itens na janela (pré-dedup).\")\n",
        "        items.extend(lang_items)\n",
        "\n",
        "    items = _dedup(items)[:MAX_NEWS_TOTAL_FOR_LLM]\n",
        "    return items\n",
        "\n",
        "def agente_noticias(empresa: str, ticker: str, setor: str,\n",
        "                    ref_cap: dt.date, lookback_days: int,\n",
        "                    debug: Dict[str, Any]) -> Tuple[str, int]:\n",
        "    t0 = time.time()\n",
        "    expand = _llm_expand_aliases_and_subreddits(empresa, ticker, setor, debug)\n",
        "    items = _collect_news(empresa, ticker, setor, expand, ref_cap, lookback_days)\n",
        "    logger.info(f\"[Noticias] Total agregados (dedup): {len(items)}\")\n",
        "\n",
        "    if not items:\n",
        "        debug[\"noticias\"] = {\"n_items\": 0, \"elapsed_s\": round(time.time()-t0, 2)}\n",
        "        return (\"Sem notícias relevantes no período.\", 5)\n",
        "\n",
        "    _enrich_bodies_parallel(items)\n",
        "    out_items, resumo, nota = _genai_resumir_e_notar(items, empresa, ticker, setor, \"Notícias\", debug)\n",
        "    debug[\"noticias\"] = {\n",
        "        \"n_items\": len(items),\n",
        "        \"elapsed_s\": round(time.time()-t0, 2),\n",
        "        \"queries\": _build_news_queries(empresa, ticker, setor, expand)\n",
        "    }\n",
        "    return resumo, nota\n",
        "\n",
        "# ============= REDDIT (PRAW -> RSS) =============\n",
        "def _reddit_client() -> Optional[Any]:\n",
        "    if praw is None:\n",
        "        logger.warning(\"[Reddit] PRAW não instalado. Use: pip install praw\")\n",
        "        return None\n",
        "    cid = _get_secret(\"REDDIT_CLIENT_ID\")\n",
        "    csecret = _get_secret(\"REDDIT_CLIENT_SECRET\")\n",
        "    uagent = _get_secret(\"REDDIT_USER_AGENT\") or USER_AGENT\n",
        "    if not (cid and csecret and uagent):\n",
        "        logger.warning(\"[Reddit] Credenciais ausentes. Set REDDIT_CLIENT_ID/SECRET/USER_AGENT.\")\n",
        "        return None\n",
        "    try:\n",
        "        reddit = praw.Reddit(\n",
        "            client_id=cid, client_secret=csecret, user_agent=uagent, check_for_async=False\n",
        "        )\n",
        "        reddit.read_only = True\n",
        "        return reddit\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"[Reddit] PRAW init falhou: {e}\")\n",
        "        return None\n",
        "\n",
        "def _collect_from_reddit_praw(queries: List[str], subreddits_incl: List[str],\n",
        "                              limit_total: int = MAX_REDDIT_ITEMS) -> List[dict]:\n",
        "    reddit = _reddit_client()\n",
        "    if reddit is None:\n",
        "        return []\n",
        "    rows = []\n",
        "    per_sr = max(REDDIT_FETCH_PER_SR_MIN, limit_total // max(1, len(subreddits_incl)))\n",
        "    for sr in subreddits_incl:\n",
        "        try:\n",
        "            sub = reddit.subreddit(sr)\n",
        "        except Exception:\n",
        "            continue\n",
        "        for q in queries[:6]:\n",
        "            try:\n",
        "                subs = sub.search(\n",
        "                    query=q, sort=\"new\", time_filter=\"all\", syntax=\"lucene\", limit=per_sr\n",
        "                )\n",
        "                for s in subs:\n",
        "                    rows.append({\n",
        "                        \"data\": {\n",
        "                            \"title\": s.title or \"\",\n",
        "                            \"permalink\": getattr(s, \"permalink\", \"\") or \"\",\n",
        "                            \"url\": getattr(s, \"url\", \"\") or \"\",\n",
        "                            \"created_utc\": int(getattr(s, \"created_utc\", 0)) or None,\n",
        "                            \"selftext\": (getattr(s, \"selftext\", \"\") or \"\")[:MAX_TXT_CHARS],\n",
        "                            \"subreddit\": str(getattr(s, \"subreddit\", \"\")) or \"\",\n",
        "                            \"score\": int(getattr(s, \"score\", 0)) or 0\n",
        "                        }\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"[Reddit] Erro r/{sr} q='{q}': {e}\")\n",
        "                continue\n",
        "    logger.info(f\"[Reddit] PRAW coletou {len(rows)} itens.\")\n",
        "    return rows\n",
        "\n",
        "def _collect_from_reddit_rss(queries: List[str], subreddits_incl: List[str], limit_per_q: int = 80) -> List[dict]:\n",
        "    out = []\n",
        "    # busca global\n",
        "    for q in queries[:6]:\n",
        "        rss_url = f\"https://www.reddit.com/search.rss?q={urllib.parse.quote_plus(q)}&sort=new\"\n",
        "        try:\n",
        "            feed = feedparser.parse(rss_url)\n",
        "        except Exception:\n",
        "            continue\n",
        "        for e in feed.entries[:limit_per_q]:\n",
        "            created_iso = None\n",
        "            if getattr(e, \"published_parsed\", None):\n",
        "                created_iso = dt.datetime(*e.published_parsed[:6], tzinfo=dt.timezone.utc).isoformat()\n",
        "            out.append({\n",
        "                \"data\": {\n",
        "                    \"title\": e.get(\"title\",\"\"),\n",
        "                    \"permalink\": \"\",\n",
        "                    \"url\": e.get(\"link\",\"\"),\n",
        "                    \"created_utc\": int(_parse_dt_iso(created_iso).timestamp()) if created_iso else None,\n",
        "                    \"selftext\": (e.get(\"summary\") or \"\")[:MAX_TXT_CHARS],\n",
        "                    \"subreddit\": \"\",\n",
        "                    \"score\": 0\n",
        "                }\n",
        "            })\n",
        "    # busca por subreddit específico via RSS\n",
        "    for sr in subreddits_incl[:12]:\n",
        "        rss_url = f\"https://www.reddit.com/r/{sr}/search.rss?q={urllib.parse.quote_plus(' OR '.join(queries[:3]))}&restrict_sr=1&sort=new\"\n",
        "        try:\n",
        "            feed = feedparser.parse(rss_url)\n",
        "        except Exception:\n",
        "            continue\n",
        "        for e in feed.entries[:limit_per_q//2]:\n",
        "            created_iso = None\n",
        "            if getattr(e, \"published_parsed\", None):\n",
        "                created_iso = dt.datetime(*e.published_parsed[:6], tzinfo=dt.timezone.utc).isoformat()\n",
        "            out.append({\n",
        "                \"data\": {\n",
        "                    \"title\": e.get(\"title\",\"\"),\n",
        "                    \"permalink\": \"\",\n",
        "                    \"url\": e.get(\"link\",\"\"),\n",
        "                    \"created_utc\": int(_parse_dt_iso(created_iso).timestamp()) if created_iso else None,\n",
        "                    \"selftext\": (e.get(\"summary\") or \"\")[:MAX_TXT_CHARS],\n",
        "                    \"subreddit\": sr,\n",
        "                    \"score\": 0\n",
        "                }\n",
        "            })\n",
        "    logger.info(f\"[Reddit] RSS coletou {len(out)} itens.\")\n",
        "    return out\n",
        "\n",
        "def _build_reddit_queries(empresa: str, ticker: str, setor: str, expand: LLMExpand) -> Tuple[List[str], List[str]]:\n",
        "    aliases: Set[str] = set([empresa, ticker] + _us_ticker_variants(ticker))\n",
        "    aliases.update([a for a in expand.aliases or [] if a])\n",
        "    for arr in (expand.translations or {}).values():\n",
        "        aliases.update(arr or [])\n",
        "    aliases = {a.strip() for a in aliases if a and len(a.strip()) >= 2}\n",
        "\n",
        "    terms = expand.focus_terms or []\n",
        "    core = \" OR \".join([f\"\\\"{a}\\\"\" if \" \" in a else a for a in sorted(aliases)])\n",
        "    queries = [core]\n",
        "    for t in terms[:10]:\n",
        "        queries.append(f\"({core}) {t}\")\n",
        "\n",
        "    base_sr = set(DEFAULT_SUBREDDITS)\n",
        "    base_sr.update(SECTOR_SUBREDDITS.get(setor, []))\n",
        "    for sr in (expand.subreddits or []):\n",
        "        if sr: base_sr.add(sr.strip().lstrip(\"r/\"))\n",
        "    subreddits = [s for s in sorted(base_sr) if re.match(r\"^[A-Za-z0-9_][A-Za-z0-9_]*$\", s)]\n",
        "\n",
        "    return queries[:8], subreddits[:30]\n",
        "\n",
        "def _enrich_reddit_items(items: List[Item]) -> None:\n",
        "    def _worker(it: Item) -> str:\n",
        "        if (not it.summary) and it.url and (\"reddit.com\" not in it.url.lower()):\n",
        "            return _extract_text_general(it.url)\n",
        "        return it.summary or \"\"\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = {ex.submit(_worker, it): it for it in items}\n",
        "        for fut in as_completed(futs):\n",
        "            it = futs[fut]\n",
        "            try:\n",
        "                body = fut.result() or \"\"\n",
        "            except Exception:\n",
        "                body = \"\"\n",
        "            if body:\n",
        "                it.summary = body[:MAX_TXT_CHARS]\n",
        "\n",
        "def agente_reddit(empresa: str, ticker: str, setor: str,\n",
        "                  ref_cap: dt.date, lookback_days: int,\n",
        "                  debug: Dict[str, Any]) -> Tuple[str, int]:\n",
        "    expand = _llm_expand_aliases_and_subreddits(empresa, ticker, setor, debug)\n",
        "    queries, subreddits = _build_reddit_queries(empresa, ticker, setor, expand)\n",
        "    debug[\"reddit_query_meta\"] = {\"queries\": queries, \"subreddits\": subreddits}\n",
        "\n",
        "    rows = _collect_from_reddit_praw(queries, subreddits, limit_total=MAX_REDDIT_ITEMS)\n",
        "    if not rows:\n",
        "        logger.warning(\"[Reddit] Sem PRAW (ou sem credenciais) ou sem resultados. Usando fallback RSS.\")\n",
        "        rows = _collect_from_reddit_rss(queries, subreddits, limit_per_q=90)\n",
        "\n",
        "    items: List[Item] = []\n",
        "    for ch in rows:\n",
        "        d = ch.get(\"data\", {})\n",
        "        title = d.get(\"title\") or \"\"\n",
        "        permalink = d.get(\"permalink\", \"\")\n",
        "        link = (\"https://www.reddit.com\" + permalink) if permalink else (d.get(\"url\", \"\") or \"\")\n",
        "        ts = d.get(\"created_utc\")\n",
        "        published_at = dt.datetime.fromtimestamp(ts, tz=dt.timezone.utc).isoformat() if ts else None\n",
        "        body = d.get(\"selftext\") or \"\"\n",
        "        it = Item(title=title, url=link, published_at=published_at, summary=body[:MAX_TXT_CHARS])\n",
        "        if _within_window(it.published_at, ref_cap, lookback_days):\n",
        "            items.append(it)\n",
        "\n",
        "    before = len(items)\n",
        "    items = _dedup(items)\n",
        "    logger.info(f\"[Reddit] Itens filtrados na janela: {len(items)} (antes dedup: {before})\")\n",
        "\n",
        "    if not items:\n",
        "        debug[\"reddit\"] = {\"n_items\": 0}\n",
        "        return (\"Sem menções relevantes no Reddit no período.\", 5)\n",
        "\n",
        "    _enrich_reddit_items(items)\n",
        "    out_items, resumo, nota = _genai_resumir_e_notar(items, empresa, ticker, setor, \"Reddit\", debug)\n",
        "    debug[\"reddit\"] = {\"n_items\": len(items)}\n",
        "    return (resumo, nota)\n",
        "\n",
        "# ============= RELATÓRIOS (SEC/CVM + ADR + XBRL) =============\n",
        "def _ticker_eh_br(ticker: str) -> bool:\n",
        "    return bool(re.search(r\"\\d$\", ticker.strip().upper()))\n",
        "\n",
        "def _sec_get_cik(ticker: str) -> Optional[str]:\n",
        "    \"\"\"Obtém CIK via company_tickers.json. Se falhar, tenta variantes de classe.\"\"\"\n",
        "    try:\n",
        "        with httpx.Client(headers={\"User-Agent\": USER_AGENT}, timeout=HTTP_TIMEOUT) as cli:\n",
        "            r = cli.get(\"https://www.sec.gov/files/company_tickers.json\")\n",
        "            if r.status_code != 200:\n",
        "                return None\n",
        "            data = r.json()\n",
        "    except Exception:\n",
        "        return None\n",
        "    up = ticker.upper()\n",
        "    for _, rec in data.items():\n",
        "        if rec.get(\"ticker\", \"\").upper() == up:\n",
        "            return f\"{int(rec['cik_str']):010d}\"\n",
        "    # tenta variantes de classe (GOOG/GOOGL etc.)\n",
        "    for var in _us_ticker_variants(up):\n",
        "        if var == up:\n",
        "            continue\n",
        "        for _, rec in data.items():\n",
        "            if rec.get(\"ticker\", \"\").upper() == var:\n",
        "                return f\"{int(rec['cik_str']):010d}\"\n",
        "    return None\n",
        "\n",
        "def _sec_search_filings_efts(ticker: str, form_types: List[str], size: int = 60) -> List[dict]:\n",
        "    body = {\n",
        "        \"query\": { \"query_string\": { \"query\": f'ticker:{ticker.upper()} AND ( ' + \" OR \".join([f'formType:\\\"{f}\\\"' for f in form_types]) + \" )\" } },\n",
        "        \"from\": 0, \"size\": size,\n",
        "        \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n",
        "    }\n",
        "    headers = {\"User-Agent\": USER_AGENT, \"Content-Type\": \"application/json\"}\n",
        "    try:\n",
        "        with httpx.Client(headers=headers, timeout=HTTP_TIMEOUT) as cli:\n",
        "            r = cli.post(\"https://efts.sec.gov/LATEST/search-index\", json=body)\n",
        "            if r.status_code != 200:\n",
        "                logger.info(f\"[SEC search-index] Falha {r.status_code}\")\n",
        "                return []\n",
        "            hits = r.json().get(\"hits\", {}).get(\"hits\", [])\n",
        "            out = []\n",
        "            for h in hits:\n",
        "                src = h.get(\"_source\", {})\n",
        "                out.append({\n",
        "                    \"adsh\": src.get(\"adsh\") or src.get(\"accNo\") or \"\",\n",
        "                    \"filedAt\": src.get(\"filedAt\"),\n",
        "                    \"cik\": str(src.get(\"cik\") or \"\"),\n",
        "                    \"formType\": src.get(\"formType\"),\n",
        "                })\n",
        "            return out\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def _sec_filing_index(cik: str, accession: str) -> dict:\n",
        "    acc = accession.replace(\"-\", \"\")\n",
        "    url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc}/index.json\"\n",
        "    try:\n",
        "        with httpx.Client(headers={\"User-Agent\": USER_AGENT}, timeout=HTTP_TIMEOUT) as cli:\n",
        "            r = cli.get(url)\n",
        "            if r.status_code != 200:\n",
        "                return {}\n",
        "            return r.json()\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def _sec_companyfacts(cik: str) -> dict:\n",
        "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{int(cik):010d}.json\"\n",
        "    try:\n",
        "        with httpx.Client(headers={\"User-Agent\": USER_AGENT}, timeout=HTTP_TIMEOUT) as cli:\n",
        "            r = cli.get(url)\n",
        "            if r.status_code != 200:\n",
        "                return {}\n",
        "            return r.json()\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def _xbrl_latest_value(series: List[dict], ref_cap: dt.date) -> Optional[Tuple[str,float,str]]:\n",
        "    best = None; best_dt = None\n",
        "    for row in series or []:\n",
        "        d = row.get(\"end\") or row.get(\"fy\")\n",
        "        if not d:\n",
        "            continue\n",
        "        try:\n",
        "            dd = dt.date.fromisoformat(str(d)[:10])\n",
        "        except Exception:\n",
        "            continue\n",
        "        if dd <= ref_cap and (best is None or dd > best_dt):\n",
        "            val = row.get(\"val\")\n",
        "            best = (str(dd), float(val) if val is not None else 0.0); best_dt = dd\n",
        "    if not best:\n",
        "        return None\n",
        "    return (best[0], best[1], \"\")\n",
        "\n",
        "def _xbrl_pick_unit(units_dict: Dict[str, List[dict]]) -> Tuple[str,List[dict]]:\n",
        "    if \"USD\" in units_dict:\n",
        "        return \"USD\", units_dict[\"USD\"]\n",
        "    for k, v in units_dict.items():\n",
        "        if isinstance(v, list) and v:\n",
        "            return k, v\n",
        "    return \"\", []\n",
        "\n",
        "def _sec_kpis(cik: str, ref_cap: dt.date) -> Dict[str, Tuple[str,float,str]]:\n",
        "    cf = _sec_companyfacts(cik)\n",
        "    out: Dict[str, Tuple[str,float,str]] = {}\n",
        "    facts = cf.get(\"facts\", {}).get(\"us-gaap\", {})\n",
        "    concepts = {\n",
        "        \"Revenue\": \"Revenues\",\n",
        "        \"NetIncome\": \"NetIncomeLoss\",\n",
        "        \"EPS_Diluted\": \"EarningsPerShareDiluted\",\n",
        "        \"OCF\": \"NetCashProvidedByUsedInOperatingActivities\",\n",
        "    }\n",
        "    for label, concept in concepts.items():\n",
        "        try:\n",
        "            units = facts[concept][\"units\"]\n",
        "        except Exception:\n",
        "            continue\n",
        "        unit_name, series = _xbrl_pick_unit(units)\n",
        "        if not series:\n",
        "            continue\n",
        "        val = _xbrl_latest_value(series, ref_cap)\n",
        "        if val:\n",
        "            out[label] = (val[0], val[1], unit_name)\n",
        "    return out\n",
        "\n",
        "def _choose_best_attachment(index_json: dict) -> Optional[str]:\n",
        "    items = (index_json.get(\"directory\", {}) or {}).get(\"item\", [])\n",
        "    if not items:\n",
        "        return None\n",
        "    def _build_url(name: str) -> str:\n",
        "        dirurl = index_json.get(\"directory\", {}).get(\"url\", \"\")\n",
        "        if dirurl and not dirurl.endswith(\"/\"):\n",
        "            dirurl += \"/\"\n",
        "        return dirurl + name if dirurl else name\n",
        "\n",
        "    ex99 = [f for f in items if str(f.get(\"name\",\"\")).upper().startswith(\"EX-99\")]\n",
        "    if ex99:\n",
        "        return _build_url(ex99[0][\"name\"])\n",
        "    htmls = [f for f in items if str(f.get(\"name\",\"\")).lower().endswith((\".htm\",\".html\"))]\n",
        "    if htmls:\n",
        "        return _build_url(htmls[0][\"name\"])\n",
        "    pdfs = [f for f in items if str(f.get(\"name\",\"\")).lower().endswith(\".pdf\")]\n",
        "    if pdfs:\n",
        "        return _build_url(pdfs[0][\"name\"])\n",
        "    primaries = [f for f in items if str(f.get(\"name\",\"\")).lower().startswith(\"primary\")]\n",
        "    if primaries:\n",
        "        return _build_url(primaries[0][\"name\"])\n",
        "    return _build_url(items[0][\"name\"])\n",
        "\n",
        "def _sec_collect_best_item(ticker: str, empresa: str, ref_cap: dt.date) -> Optional[Item]:\n",
        "    form_priority = [\"10-Q\",\"10-K\",\"6-K\",\"20-F\",\"8-K\"]\n",
        "    # 1) efts search-index\n",
        "    hits = _sec_search_filings_efts(ticker, form_priority, size=60)\n",
        "    if not hits:\n",
        "        # 1b) tentar variantes de classe\n",
        "        for var in _us_ticker_variants(ticker):\n",
        "            if var == ticker:\n",
        "                continue\n",
        "            hits = _sec_search_filings_efts(var, form_priority, size=60)\n",
        "            if hits:\n",
        "                break\n",
        "    if not hits:\n",
        "        # 2) submissions fallback\n",
        "        cik = _sec_get_cik(ticker)\n",
        "        if not cik:\n",
        "            return None\n",
        "        sub_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "        try:\n",
        "            with httpx.Client(headers={\"User-Agent\": USER_AGENT}, timeout=HTTP_TIMEOUT) as cli:\n",
        "                r = cli.get(sub_url)\n",
        "                if r.status_code != 200:\n",
        "                    return None\n",
        "                sub = r.json()\n",
        "        except Exception:\n",
        "            return None\n",
        "        forms = sub.get(\"filings\", {}).get(\"recent\", {})\n",
        "        items: List[Item] = []\n",
        "        for i, ftype in enumerate(forms.get(\"form\", [])):\n",
        "            if ftype not in set(form_priority):\n",
        "                continue\n",
        "            fdate = forms[\"filingDate\"][i]\n",
        "            acc = forms[\"accessionNumber\"][i].replace(\"-\", \"\")\n",
        "            prim = forms[\"primaryDocument\"][i]\n",
        "            doc_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc}/{prim}\"\n",
        "            items.append(Item(\n",
        "                title=f\"{ftype} {empresa} ({fdate})\",\n",
        "                url=doc_url,\n",
        "                published_at=fdate,\n",
        "                summary=\"\"\n",
        "            ))\n",
        "        latest = _pick_latest_leq(items, ref_cap)\n",
        "        return latest\n",
        "\n",
        "    def _d(h):\n",
        "        return _parse_dt_iso(h.get(\"filedAt\") or \"\") or dt.datetime.min.replace(tzinfo=dt.timezone.utc)\n",
        "    hits = [h for h in hits if _d(h).date() <= ref_cap]\n",
        "    if not hits:\n",
        "        return None\n",
        "    pri = {f:i for i,f in enumerate(form_priority)}\n",
        "    hits.sort(key=lambda h: (pri.get(h.get(\"formType\",\"ZZZ\"), 999), _d(h)), reverse=False)\n",
        "    best_form = hits[0][\"formType\"]\n",
        "    cand = [h for h in hits if h[\"formType\"] == best_form]\n",
        "    cand.sort(key=lambda h: _d(h), reverse=True)\n",
        "    chosen = cand[0]\n",
        "    cik = chosen.get(\"cik\") or _sec_get_cik(ticker)\n",
        "    adsh = chosen.get(\"adsh\",\"\")\n",
        "    if not cik or not adsh:\n",
        "        return None\n",
        "\n",
        "    idx = _sec_filing_index(cik, adsh)\n",
        "    best_url = _choose_best_attachment(idx)\n",
        "    filed_date = (chosen.get(\"filedAt\") or \"\")[:10] or None\n",
        "    title = f\"{best_form} {empresa} ({filed_date or 'N/D'})\"\n",
        "    summary = _extract_text_general(best_url) if best_url else \"\"\n",
        "    return Item(title=title, url=best_url or \"\", published_at=filed_date, summary=summary)\n",
        "\n",
        "def _cvm_rows_for_year(year: int, kind: str) -> Optional[List[List[str]]]:\n",
        "    kind = kind.lower()\n",
        "    url = f\"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/{kind.upper()}/DADOS/{kind}_cia_aberta_{year}.zip\"\n",
        "    try:\n",
        "        with httpx.Client(headers={\"User-Agent\": USER_AGENT}, timeout=HTTP_TIMEOUT) as cli:\n",
        "            r = cli.get(url)\n",
        "            if r.status_code != 200:\n",
        "                return None\n",
        "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "            name = next((n for n in z.namelist() if n.lower().endswith(\".csv\") and kind in n.lower() and \"cia_aberta\" in n.lower()), None)\n",
        "            if not name:\n",
        "                return None\n",
        "            raw = z.read(name).decode(\"utf-8\", errors=\"ignore\")\n",
        "            rows = list(csv.reader(io.StringIO(raw), delimiter=';', quotechar='\"'))\n",
        "        return rows\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _find_cols(header: List[str], *alts) -> Optional[int]:\n",
        "    header = [h.strip().lower() for h in header]\n",
        "    for a in alts:\n",
        "        if a in header:\n",
        "            return header.index(a)\n",
        "    return None\n",
        "\n",
        "def _guess_aliases_br(ticker: str, empresa: str) -> List[str]:\n",
        "    base = re.sub(r\"\\d+$\", \"\", (ticker or \"\").upper())\n",
        "    norm_emp = _normalize_name(empresa)\n",
        "    aliases = {norm_emp, empresa, ticker.upper()}\n",
        "    common = {\n",
        "        \"PETR\": \"PETROLEO BRASILEIRO S A PETROBRAS\",\n",
        "        \"VALE\": \"VALE S A\",\n",
        "        \"BBDC\": \"BANCO BRADESCO S A\",\n",
        "        \"ITUB\": \"ITAU UNIBANCO HOLDING S A\",\n",
        "        \"BBAS\": \"BANCO DO BRASIL S A\",\n",
        "        \"ABEV\": \"AMBEV S A\",\n",
        "        \"WEGE\": \"WEG S A\",\n",
        "        \"ELET\": \"CENTRAIS ELETRICAS BRASILEIRAS S A ELETROBRAS\",\n",
        "        \"SUZB\": \"SUZANO S A\",\n",
        "    }\n",
        "    if base in common:\n",
        "        aliases.add(common[base])\n",
        "    if norm_emp.endswith(\" S A\"):\n",
        "        aliases.add(norm_emp.replace(\" S A\", \"\"))\n",
        "    return list(aliases)\n",
        "\n",
        "def _cvm_collect_items(empresa: str, ticker: str, ref_cap: dt.date) -> List[Item]:\n",
        "    items: List[Item] = []\n",
        "    years = {ref_cap.year, ref_cap.year - 1}\n",
        "    aliases = _guess_aliases_br(ticker, empresa)\n",
        "\n",
        "    def _collect_one(kind: str, y: int) -> List[Item]:\n",
        "        rows = _cvm_rows_for_year(y, kind)\n",
        "        out: List[Item] = []\n",
        "        if not rows or len(rows) < 2:\n",
        "            return out\n",
        "        header = [c.strip().lower() for c in rows[0]]\n",
        "        i_nome = _find_cols(header, \"denom_cia\",\"nome_cia\",\"nome_empresarial\")\n",
        "        i_data = _find_cols(header, \"dt_refer\",\"data_refer\",\"dt_referencias\")\n",
        "        i_link = _find_cols(header, \"link_doc\",\"link_documento\",\"end_doc\")\n",
        "        if i_nome is None:\n",
        "            return out\n",
        "        for cols in rows[1:]:\n",
        "            row_name = (cols[i_nome] if i_nome < len(cols) else \"\") or \"\"\n",
        "            norm_row = _normalize_name(row_name)\n",
        "            if not _name_matches(norm_row, aliases):\n",
        "                continue\n",
        "            dt_ref = None; display = str(y)\n",
        "            if i_data is not None and i_data < len(cols) and (cols[i_data] or \"\").strip():\n",
        "                s_val = cols[i_data].strip()\n",
        "                display = s_val\n",
        "                try:\n",
        "                    dt_ref = dt.date.fromisoformat(s_val)\n",
        "                except Exception:\n",
        "                    try:\n",
        "                        d,m,a = s_val.split(\"/\")\n",
        "                        dt_ref = dt.date(int(a), int(m), int(d))\n",
        "                    except Exception:\n",
        "                        dt_ref = None\n",
        "            url_doc = cols[i_link] if (i_link is not None and i_link < len(cols)) else \"\"\n",
        "            out.append(Item(\n",
        "                title=f\"{kind.upper()} {row_name} ({display})\",\n",
        "                url=url_doc,\n",
        "                published_at=dt_ref.isoformat() if dt_ref else None,\n",
        "                summary=\"\"\n",
        "            ))\n",
        "        return out\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=min(4, len(years)*2)) as ex:\n",
        "        jobs = [ex.submit(_collect_one, kind, y) for kind in (\"itr\", \"dfp\") for y in sorted(years, reverse=True)]\n",
        "        for fut in as_completed(jobs):\n",
        "            items.extend(fut.result() or [])\n",
        "\n",
        "    logger.info(f\"[Relatorios][CVM] '{empresa}': coletados {len(items)} candidatos (ITR+DFP).\")\n",
        "    return items\n",
        "\n",
        "def _pick_latest_leq(items: List[Item], ref_cap: dt.date) -> Optional[Item]:\n",
        "    best = None; best_date = None\n",
        "    for it in items:\n",
        "        t = _parse_dt_iso(it.published_at)\n",
        "        if not t:\n",
        "            continue\n",
        "        d = t.date()\n",
        "        if d <= ref_cap and (best is None or d > best_date):\n",
        "            best = it; best_date = d\n",
        "    return best\n",
        "\n",
        "def _extract_tickers_from_llm_aliases(expand: LLMExpand) -> List[str]:\n",
        "    \"\"\"Extrai candidatos a tickers US/ADR a partir de aliases do LLM (heurística).\"\"\"\n",
        "    cands = set()\n",
        "    for a in (expand.aliases or []):\n",
        "        a = a.strip().upper()\n",
        "        if re.match(r\"^[A-Z]{1,5}([.\\-][A-Z])?$\", a):\n",
        "            cands.add(a)\n",
        "    return list(cands)\n",
        "\n",
        "def _try_sec_with_adr_fallback(empresa: str, ticker_br: str, ref_cap: dt.date, debug: Dict[str, Any]) -> Optional[Item]:\n",
        "    # 1) seeds mínimos por mapa\n",
        "    tried = set()\n",
        "    base = re.sub(r\"\\d+$\", \"\", (ticker_br or \"\").upper())\n",
        "    for adr in BR_TO_ADR_SEED.get(base, []):\n",
        "        tried.add(adr)\n",
        "        logger.info(f\"[Relatorios][SEC] Tentando ADR seed: {adr}\")\n",
        "        it = _sec_collect_best_item(adr, empresa, ref_cap)\n",
        "        if it:\n",
        "            debug.setdefault(\"relatorios\", {})[\"adr_fallback\"] = {\"via\":\"seed\", \"ticker\": adr}\n",
        "            return it\n",
        "    # 2) usar LLM para sugerir tickers (aliases → regex de ticker)\n",
        "    try:\n",
        "        expand = _llm_expand_aliases_and_subreddits(empresa, ticker_br, \"BR_ADR\", debug)\n",
        "    except Exception:\n",
        "        expand = LLMExpand()\n",
        "    llm_cands = [t for t in _extract_tickers_from_llm_aliases(expand) if t not in tried]\n",
        "    for adr in llm_cands[:6]:\n",
        "        logger.info(f\"[Relatorios][SEC] Tentando ADR via LLM: {adr}\")\n",
        "        it = _sec_collect_best_item(adr, empresa, ref_cap)\n",
        "        if it:\n",
        "            debug.setdefault(\"relatorios\", {})[\"adr_fallback\"] = {\"via\":\"llm\", \"ticker\": adr}\n",
        "            return it\n",
        "    return None\n",
        "\n",
        "def agente_relatorios(empresa: str, ticker: str, setor: str,\n",
        "                      ref_cap: dt.date, debug: Dict[str, Any]) -> Tuple[str, int]:\n",
        "    t0 = time.time()\n",
        "\n",
        "    if _ticker_eh_br(ticker):\n",
        "        candidatos = _cvm_collect_items(empresa, ticker, ref_cap)\n",
        "        latest = _pick_latest_leq(candidatos, ref_cap)\n",
        "        if not latest:\n",
        "            logger.info(f\"[Relatorios] CVM vazia para {ticker}. Tentando SEC via ADR...\")\n",
        "            latest = _try_sec_with_adr_fallback(empresa, ticker, ref_cap, debug)\n",
        "\n",
        "        if not latest:\n",
        "            debug[\"relatorios\"] = {\n",
        "                \"selected\": None,\n",
        "                \"n_candidates\": len(candidatos),\n",
        "                \"elapsed_s\": round(time.time()-t0, 2)\n",
        "            }\n",
        "            return (\"Sem relatórios elegíveis no período.\", 5)\n",
        "\n",
        "        if latest.url and not latest.summary:\n",
        "            latest.summary = _extract_text_general(latest.url)\n",
        "\n",
        "        debug[\"relatorios\"] = {\n",
        "            \"selected\": latest.title,\n",
        "            \"date\": latest.published_at,\n",
        "            \"n_candidates\": len(candidatos),\n",
        "            \"elapsed_s\": round(time.time()-t0, 2)\n",
        "        }\n",
        "        out_items, resumo, nota = _genai_resumir_e_notar([latest], empresa, ticker, setor, \"Relatórios\", debug)\n",
        "        return resumo, nota\n",
        "\n",
        "    else:\n",
        "        best = _sec_collect_best_item(ticker, empresa, ref_cap)\n",
        "        if not best:\n",
        "            for var in _us_ticker_variants(ticker):\n",
        "                if var != ticker:\n",
        "                    best = _sec_collect_best_item(var, empresa, ref_cap)\n",
        "                    if best:\n",
        "                        break\n",
        "        if not best:\n",
        "            debug[\"relatorios\"] = {\n",
        "                \"selected\": None,\n",
        "                \"n_candidates\": 0,\n",
        "                \"elapsed_s\": round(time.time()-t0, 2)\n",
        "            }\n",
        "            return (\"Sem relatórios elegíveis no período.\", 5)\n",
        "\n",
        "        cik = _sec_get_cik(ticker) or _sec_get_cik(_us_ticker_variants(ticker)[0])\n",
        "        kpis = _sec_kpis(cik, ref_cap) if cik else {}\n",
        "\n",
        "        debug[\"relatorios\"] = {\n",
        "            \"selected\": best.title,\n",
        "            \"date\": best.published_at,\n",
        "            \"n_candidates\": 1,\n",
        "            \"elapsed_s\": round(time.time()-t0, 2),\n",
        "            \"xbrl_kpis\": kpis\n",
        "        }\n",
        "        out_items, resumo, nota = _genai_resumir_e_notar([best], empresa, ticker, setor, \"Relatórios\", debug, kpis=kpis)\n",
        "        return resumo, nota\n",
        "\n",
        "# ============= FORMATAÇÃO / ORQUESTRAÇÃO =============\n",
        "def _bloco_fonte(resumo: str, nota: int) -> dict:\n",
        "    return {\"Empresa\": {\"resumo\": resumo, \"nota\": nota}}\n",
        "\n",
        "def _analise_para_data(empresa: str, ticker: str, setor: str, data_ref_str: str, lookback_days: int) -> dict:\n",
        "    ref_date = _to_date(data_ref_str)\n",
        "    ref_cap  = _cap_ref_date(ref_date)\n",
        "    debug: Dict[str, Any] = {\"ref_date\": str(ref_date), \"ref_cap\": str(ref_cap)}\n",
        "\n",
        "    resumo_rel, nota_rel = agente_relatorios(empresa, ticker, setor, ref_cap, debug)\n",
        "    resumo_not, nota_not = agente_noticias (empresa, ticker, setor, ref_cap, lookback_days, debug)\n",
        "    resumo_red, nota_red = agente_reddit   (empresa, ticker, setor, ref_cap, lookback_days, debug)\n",
        "\n",
        "    out = {\n",
        "        \"data_de_referencia\": data_ref_str,\n",
        "        \"Relatorios\": _bloco_fonte(resumo_rel, nota_rel),\n",
        "        \"Noticias\":   _bloco_fonte(resumo_not, nota_not),\n",
        "        \"Reddit\":     _bloco_fonte(resumo_red, nota_red),\n",
        "        \"_debug\":     debug\n",
        "    }\n",
        "    return out\n",
        "\n",
        "def pesquisa_ação(ação: dict, lookback_days: int = 30) -> dict:\n",
        "    \"\"\"\n",
        "    ação = {\n",
        "        \"codigo_da_ação\": \"PETR3\"  # BR com dígito -> CVM; US sem dígito -> SEC\n",
        "        \"Nome_da_empresa\": \"Petrobras - Petróleo Brasileiro S.A.\",\n",
        "        \"Setor_da_empresa\": \"Energy\",\n",
        "        \"Final_do_teste\": \"2025-06-30\",\n",
        "        \"hojé_em_dia\": \"2025-12-31\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    ticker = ação.get(\"codigo_da_ação\") or ação.get(\"codigo_da_acao\") or \"\"\n",
        "    empresa = ação.get(\"Nome_da_empresa\") or ação.get(\"nome_da_empresa\") or \"\"\n",
        "    setor   = ação.get(\"Setor_da_empresa\") or ação.get(\"setor_da_empresa\") or \"\"\n",
        "    data_final_teste = ação.get(\"Final_do_teste\")\n",
        "    data_hoje_str    = ação.get(\"hojé_em_dia\") or ação.get(\"hoje_em_dia\")\n",
        "\n",
        "    if not (ticker and empresa and setor and data_final_teste and data_hoje_str):\n",
        "        raise ValueError(\"Entradas faltando: requer 'codigo_da_ação', 'Nome_da_empresa', 'Setor_da_empresa', 'Final_do_teste', 'hojé_em_dia'.\")\n",
        "\n",
        "    print(f\">>> Rodando pesquisa_ação com DEBUG ... (threads={MAX_WORKERS})\")\n",
        "    t0 = time.time()\n",
        "    analise_hoje  = _analise_para_data(empresa, ticker, setor, data_hoje_str, lookback_days)\n",
        "    analise_final = _analise_para_data(empresa, ticker, setor, data_final_teste, lookback_days)\n",
        "    elapsed = round(time.time()-t0, 2)\n",
        "    print(f\">>> Concluído em {elapsed}s.\")\n",
        "\n",
        "    saida = {\n",
        "        \"informacoes_da_empresa\": {\n",
        "            \"codigo_da_acao\": ticker,\n",
        "            \"nome_da_empresa\": empresa\n",
        "        },\n",
        "        \"analise_de_sentimentos\": {\n",
        "            \"hoje_em_dia\": analise_hoje,\n",
        "            \"Final_do_teste\": analise_final\n",
        "        },\n",
        "        \"_meta\": {\"elapsed_s\": elapsed, \"threads\": MAX_WORKERS}\n",
        "    }\n",
        "    return saida\n",
        "\n",
        "# ============= EXEMPLO RÁPIDO =============\n",
        "if __name__ == \"__main__\":\n",
        "    exemplo = {\n",
        "        \"codigo_da_ação\": \"PETR3\",\n",
        "        \"Nome_da_empresa\": \"Petrobras - Petróleo Brasileiro S.A.\",\n",
        "        \"Setor_da_empresa\": \"Energy\",\n",
        "        \"Final_do_teste\": \"2025-06-30\",\n",
        "        \"hojé_em_dia\": \"2025-09-06\"\n",
        "    }\n",
        "    print(json.dumps(pesquisa_ação(exemplo, lookback_days=30), ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "pesquisa_acao_daily_kpis.py — diário, rápido, KPIs corretas (US-GAAP + IFRS), sem imputação,\n",
        "sem AutoETS, com lock-to-publication e remoção de colunas vazias.\n",
        "\n",
        "Principais pontos:\n",
        "- PREÇO (yfinance; fallbacks: Stooq/Alpha Vantage se quiser).\n",
        "- KPIs por SEC CompanyFacts (us-gaap + ifrs-full; forms: 10-Q/10-K/6-K/20-F).\n",
        "- Alinhamento diário \"lock-to-publication\": a cada dia de pregão, vale o último 'filed' conhecido.\n",
        "- NENHUMA imputação: não interpolamos nem preenchemos NaNs. Apenas propagação pós-filed.\n",
        "- Colunas de KPIs 100% vazias são REMOVIDAS (e os respectivos flags também).\n",
        "- Debug detalhado e sem “spam” de warnings. Cache leve em memória/disk opcional.\n",
        "\n",
        "Variáveis de ambiente:\n",
        "- SEC_USER_AGENT (recomendado; ex.: \"SeuNome SeuEmail\").\n",
        "- ALPHAVANTAGE_API_KEY (opcional; só se ativar o uso).\n",
        "- SEC_CACHE_DIR (opcional; caminho para cache JSON da SEC).\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, io, json, hashlib, math, warnings, pathlib\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Tuple, Set\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --------- Dependências externas (todas opcionais com fallback controlado)\n",
        "try:\n",
        "    import requests\n",
        "except Exception:  # pragma: no cover\n",
        "    requests = None\n",
        "\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except Exception:  # pragma: no cover\n",
        "    yf = None\n",
        "\n",
        "\n",
        "# ===================== Utilidades de Debug =====================\n",
        "@dataclass\n",
        "class DebugLogger:\n",
        "    enabled: bool = True\n",
        "    events: List[Dict[str, Any]] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.events is None:\n",
        "            self.events = []\n",
        "\n",
        "    def _now(self) -> str:\n",
        "        return datetime.now(timezone.utc).isoformat(timespec='seconds')\n",
        "\n",
        "    def log(self, msg: str, **kv) -> None:\n",
        "        if self.enabled:\n",
        "            print(f\"[DEBUG] {msg}\")\n",
        "        self.events.append({\"t\": self._now(), \"msg\": msg, **kv})\n",
        "\n",
        "    def section(self, title: str) -> None:\n",
        "        if self.enabled:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(title)\n",
        "            print(\"=\" * 80)\n",
        "        self.events.append({\"t\": self._now(), \"section\": title})\n",
        "\n",
        "    def df(self, name: str, df: pd.DataFrame, head: int = 5) -> None:\n",
        "        if self.enabled:\n",
        "            print(f\"[DEBUG][DF] {name}: shape={df.shape}, cols={list(df.columns)}\")\n",
        "            with pd.option_context('display.max_columns', None, 'display.width', 180):\n",
        "                print(df.head(head))\n",
        "        self.events.append({\"t\": self._now(),\n",
        "                            \"df\": name, \"shape\": tuple(df.shape), \"cols\": list(df.columns)})\n",
        "\n",
        "\n",
        "# ===================== Fontes de PREÇO =====================\n",
        "def _ensure_utc(dtser: pd.Series) -> pd.Series:\n",
        "    s = pd.to_datetime(dtser, utc=True)\n",
        "    # já tz-aware\n",
        "    return s\n",
        "\n",
        "def fetch_prices_yfinance(ticker: str, logger: DebugLogger,\n",
        "                          period: str = \"max\", interval: str = \"1d\") -> Optional[pd.DataFrame]:\n",
        "    if yf is None:\n",
        "        logger.log(\"yfinance indisponível.\")\n",
        "        return None\n",
        "    try:\n",
        "        t = yf.Ticker(ticker)\n",
        "        hist = t.history(period=period, interval=interval, auto_adjust=False, actions=True)\n",
        "        if hist is None or hist.empty:\n",
        "            logger.log(\"yfinance vazio.\")\n",
        "            return None\n",
        "        # preço ajustado (Adj Close) quando existir\n",
        "        if \"Adj Close\" in hist.columns:\n",
        "            ser = hist[\"Adj Close\"].rename(\"close_adjusted\")\n",
        "        else:\n",
        "            ser = hist[\"Close\"].rename(\"close_adjusted\")\n",
        "        out = ser.reset_index().rename(columns={\"Date\": \"date\"})\n",
        "        out[\"date\"] = _ensure_utc(out[\"date\"])\n",
        "        # Dividends / Stock Splits\n",
        "        div = hist.get(\"Dividends\", pd.Series(dtype=float)).reset_index().rename(columns={\"Date\":\"date\"})\n",
        "        spl = hist.get(\"Stock Splits\", pd.Series(dtype=float)).reset_index().rename(columns={\"Date\":\"date\"})\n",
        "        if not div.empty:\n",
        "            div[\"date\"] = _ensure_utc(div[\"date\"])\n",
        "        if not spl.empty:\n",
        "            spl[\"date\"] = _ensure_utc(spl[\"date\"])\n",
        "        out = out.merge(div, on=\"date\", how=\"left\")\n",
        "        if \"Dividends\" not in out.columns:\n",
        "            out[\"Dividends\"] = np.nan\n",
        "        out = out.merge(spl, on=\"date\", how=\"left\")\n",
        "        if \"Stock Splits\" not in out.columns:\n",
        "            out[\"Stock Splits\"] = np.nan\n",
        "        out[\"source\"] = \"yfinance\"\n",
        "        logger.log(f\"yfinance OK: {ticker}, linhas={len(out)}\")\n",
        "        return out[[\"date\",\"close_adjusted\",\"Dividends\",\"Stock Splits\",\"source\"]]\n",
        "    except Exception as e:  # pragma: no cover\n",
        "        logger.log(f\"Falha yfinance: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_prices_stooq(ticker: str, logger: DebugLogger) -> Optional[pd.DataFrame]:\n",
        "    if requests is None:\n",
        "        logger.log(\"requests indisponível p/ Stooq.\")\n",
        "        return None\n",
        "    try:\n",
        "        sym = ticker.lower()\n",
        "        if not sym.endswith(\".us\"):\n",
        "            sym = f\"{sym}.us\"\n",
        "        url = f\"https://stooq.com/q/d/l/?s={sym}&i=d\"\n",
        "        r = requests.get(url, timeout=30)\n",
        "        if r.status_code != 200 or not r.text:\n",
        "            logger.log(f\"Stooq sem dados (HTTP {r.status_code}).\")\n",
        "            return None\n",
        "        df = pd.read_csv(io.StringIO(r.text))\n",
        "        if df.empty:\n",
        "            logger.log(\"Stooq vazio.\")\n",
        "            return None\n",
        "        df.rename(columns=str.lower, inplace=True)\n",
        "        out = pd.DataFrame({\n",
        "            \"date\": pd.to_datetime(df[\"date\"], utc=True),\n",
        "            \"close_adjusted\": df[\"close\"].astype(float),\n",
        "            \"Dividends\": np.nan,\n",
        "            \"Stock Splits\": np.nan,\n",
        "            \"source\": \"stooq\",\n",
        "        })\n",
        "        logger.log(f\"Stooq OK: {ticker}, linhas={len(out)}\")\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Falha Stooq: {e}\")\n",
        "        return None\n",
        "\n",
        "ALPHAVANTAGE_URL = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "def fetch_prices_alphavantage(ticker: str, logger: DebugLogger) -> Optional[pd.DataFrame]:\n",
        "    if requests is None:\n",
        "        logger.log(\"requests indisponível p/ Alpha Vantage.\")\n",
        "        return None\n",
        "    api_key = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not api_key:\n",
        "        logger.log(\"Alpha Vantage ignorado (sem API key).\")\n",
        "        return None\n",
        "    try:\n",
        "        params = {\"function\":\"TIME_SERIES_DAILY_ADJUSTED\",\"symbol\":ticker,\"outputsize\":\"full\",\"apikey\":api_key}\n",
        "        r = requests.get(ALPHAVANTAGE_URL, params=params, timeout=45)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "        key_ts = \"Time Series (Daily)\"\n",
        "        if key_ts not in js:\n",
        "            logger.log(f\"Alpha Vantage estrutura inesperada: {list(js.keys())[:5]}\")\n",
        "            return None\n",
        "        recs = []\n",
        "        for d, vals in js[key_ts].items():\n",
        "            recs.append({\n",
        "                \"date\": pd.to_datetime(d, utc=True),\n",
        "                \"close_adjusted\": float(vals.get(\"5. adjusted close\", np.nan)),\n",
        "                \"Dividends\": float(vals.get(\"7. dividend amount\", 0.0) or 0.0),\n",
        "                \"Stock Splits\": float(vals.get(\"8. split coefficient\", 0.0) or 0.0),\n",
        "                \"source\": \"alphavantage\"\n",
        "            })\n",
        "        df = pd.DataFrame(recs).sort_values(\"date\").reset_index(drop=True)\n",
        "        logger.log(f\"Alpha Vantage OK: {ticker}, linhas={len(df)}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Falha Alpha Vantage: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def stitch_price_sources(dfs: List[pd.DataFrame], logger: DebugLogger,\n",
        "                         compute_disparity: bool = True) -> pd.DataFrame:\n",
        "    dfs = [d for d in dfs if d is not None and not d.empty]\n",
        "    if not dfs:\n",
        "        raise RuntimeError(\"Nenhuma fonte de preço retornou dados.\")\n",
        "    cat = pd.concat(dfs, ignore_index=True)\n",
        "    cat[\"date\"] = pd.to_datetime(cat[\"date\"], utc=True)\n",
        "    cat = cat.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
        "    grp = cat.groupby(\"date\", as_index=True)\n",
        "\n",
        "    med = grp[\"close_adjusted\"].median().rename(\"close_adjusted\")\n",
        "    nsrc = grp[\"source\"].nunique().rename(\"n_sources\")\n",
        "\n",
        "    if compute_disparity:\n",
        "        lo = grp[\"close_adjusted\"].min()\n",
        "        hi = grp[\"close_adjusted\"].max()\n",
        "        disparity_bps = ((hi - lo) / med.replace(0, np.nan) * 10000).rename(\"disparity_bps\")\n",
        "        flag_disparity = (disparity_bps.fillna(0) > 50)  # limiar pedagógico\n",
        "    else:\n",
        "        disparity_bps = pd.Series(index=med.index, data=0.0, name=\"disparity_bps\")\n",
        "        flag_disparity = pd.Series(index=med.index, data=False, name=\"flag_disparity\")\n",
        "\n",
        "    out = pd.concat([med, nsrc, disparity_bps, flag_disparity], axis=1).reset_index()\n",
        "\n",
        "    # Dividends / Splits: preferências por fonte\n",
        "    def pick_pref(col: str) -> pd.Series:\n",
        "        w = cat.pivot_table(index=\"date\", columns=\"source\", values=col, aggfunc=\"first\")\n",
        "        for pref in [\"yfinance\", \"alphavantage\", \"stooq\"]:\n",
        "            if pref in w.columns:\n",
        "                return w[pref]\n",
        "        return pd.Series(index=out[\"date\"], dtype=float)\n",
        "\n",
        "    out = out.merge(pick_pref(\"Dividends\").rename(\"Dividends\"), on=\"date\", how=\"left\")\n",
        "    out = out.merge(pick_pref(\"Stock Splits\").rename(\"Stock Splits\"), on=\"date\", how=\"left\")\n",
        "\n",
        "    out = out.sort_values(\"date\").reset_index(drop=True)\n",
        "    logger.log(\"Stitching concluído\",\n",
        "               dias=len(out),\n",
        "               datas=f\"{out['date'].min().date()}..{out['date'].max().date()}\")\n",
        "    return out\n",
        "\n",
        "\n",
        "# ===================== SEC (companyfacts) =====================\n",
        "SEC_FACTS_URL = \"https://data.sec.gov/api/xbrl/companyfacts/CIK{CIK}.json\"\n",
        "SEC_TICKERS_JSON = \"https://www.sec.gov/files/company_tickers.json\"\n",
        "\n",
        "FORM_OK = {\"10-Q\",\"10-K\",\"6-K\",\"20-F\"}\n",
        "UNIT_PREF = [\"USD\",\"USD/shares\",\"USD/share\",\"USD per share\"]\n",
        "\n",
        "# Mapa canônico -> lista de candidatos por taxonomia (us-gaap / ifrs-full)\n",
        "CANON_MAP: Dict[str, Dict[str, List[str]]] = {\n",
        "    \"Revenues\": {\n",
        "        \"us-gaap\": [\"Revenues\",\"RevenueFromContractWithCustomerExcludingAssessedTax\",\"SalesRevenueNet\"],\n",
        "        \"ifrs-full\": [\"Revenue\",\"RevenueFromContractsWithCustomers\"]\n",
        "    },\n",
        "    \"NetIncomeLoss\": {\n",
        "        \"us-gaap\": [\"NetIncomeLoss\",\"ProfitLoss\"],\n",
        "        \"ifrs-full\": [\"ProfitLoss\"]\n",
        "    },\n",
        "    \"OperatingIncomeLoss\": {\n",
        "        \"us-gaap\": [\"OperatingIncomeLoss\",\"OperatingIncomeLossAlternative\"],\n",
        "        \"ifrs-full\": [\"OperatingProfitLoss\"]\n",
        "    },\n",
        "    \"GrossProfit\": {\n",
        "        \"us-gaap\": [\"GrossProfit\"],\n",
        "        \"ifrs-full\": [\"GrossProfit\"]\n",
        "    },\n",
        "    \"EarningsPerShareBasic\": {\n",
        "        \"us-gaap\": [\"EarningsPerShareBasic\"],\n",
        "        \"ifrs-full\": [\"BasicEarningsLossPerShare\"]\n",
        "    },\n",
        "    \"EarningsPerShareDiluted\": {\n",
        "        \"us-gaap\": [\"EarningsPerShareDiluted\"],\n",
        "        \"ifrs-full\": [\"DilutedEarningsLossPerShare\"]\n",
        "    },\n",
        "    \"Assets\": {\n",
        "        \"us-gaap\": [\"Assets\"],\n",
        "        \"ifrs-full\": [\"Assets\"]\n",
        "    },\n",
        "    \"Liabilities\": {\n",
        "        \"us-gaap\": [\"Liabilities\"],\n",
        "        \"ifrs-full\": [\"Liabilities\"]\n",
        "    },\n",
        "    \"StockholdersEquity\": {\n",
        "        \"us-gaap\": [\"StockholdersEquity\",\"StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest\"],\n",
        "        \"ifrs-full\": [\"Equity\",\"EquityAttributableToOwnersOfParent\"]\n",
        "    },\n",
        "    \"CashAndCashEquivalentsAtCarryingValue\": {\n",
        "        \"us-gaap\": [\"CashAndCashEquivalentsAtCarryingValue\"],\n",
        "        \"ifrs-full\": [\"CashAndCashEquivalents\"]\n",
        "    },\n",
        "    \"LongTermDebtNoncurrent\": {\n",
        "        \"us-gaap\": [\"LongTermDebtNoncurrent\",\"LongTermBorrowingsNoncurrent\"],\n",
        "        \"ifrs-full\": [\"NoncurrentBorrowings\",\"NoncurrentFinancialLiabilities\"]\n",
        "    },\n",
        "    \"CurrentAssets\": {\n",
        "        \"us-gaap\": [\"AssetsCurrent\"],\n",
        "        \"ifrs-full\": [\"CurrentAssets\"]\n",
        "    },\n",
        "    \"CurrentLiabilities\": {\n",
        "        \"us-gaap\": [\"LiabilitiesCurrent\"],\n",
        "        \"ifrs-full\": [\"CurrentLiabilities\"]\n",
        "    },\n",
        "    \"ResearchAndDevelopmentExpense\": {\n",
        "        \"us-gaap\": [\"ResearchAndDevelopmentExpense\"],\n",
        "        \"ifrs-full\": [\"ResearchAndDevelopmentExpense\"]\n",
        "    },\n",
        "    \"CapitalExpenditures\": {\n",
        "        \"us-gaap\": [\"CapitalExpenditures\",\"PaymentsToAcquirePropertyPlantAndEquipment\"],\n",
        "        \"ifrs-full\": [\"PaymentsToAcquirePropertyPlantAndEquipment\"]\n",
        "    },\n",
        "}\n",
        "\n",
        "# simples cache em memória/arquivo para companyfacts\n",
        "_SEC_CACHE: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "def _sec_headers() -> Dict[str,str]:\n",
        "    ua = os.getenv(\"SEC_USER_AGENT\", \"YourName Contact@example.com\")\n",
        "    return {\"User-Agent\": ua}\n",
        "\n",
        "def resolve_cik_from_ticker(ticker: str, logger: DebugLogger, session: Optional[\"requests.Session\"]=None) -> Optional[str]:\n",
        "    if requests is None:\n",
        "        logger.log(\"requests indisponível p/ SEC.\")\n",
        "        return None\n",
        "    try:\n",
        "        s = session or requests.Session()\n",
        "        r = s.get(SEC_TICKERS_JSON, headers=_sec_headers(), timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        t = ticker.upper()\n",
        "        for _, entry in data.items():\n",
        "            if entry.get(\"ticker\",\"\").upper() == t:\n",
        "                cik = str(entry[\"cik_str\"]).zfill(10)\n",
        "                logger.log(f\"CIK resolvido: {ticker}->{cik}\")\n",
        "                return cik\n",
        "        logger.log(f\"CIK não encontrado para {ticker}.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Falha resolve CIK: {e}\")\n",
        "        return None\n",
        "\n",
        "def _sec_cache_path(cik: str) -> Optional[pathlib.Path]:\n",
        "    cdir = os.getenv(\"SEC_CACHE_DIR\")\n",
        "    if not cdir:\n",
        "        return None\n",
        "    try:\n",
        "        p = pathlib.Path(cdir).expanduser().resolve()\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "        return p / f\"companyfacts_{cik}.json\"\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def fetch_companyfacts_json(cik: str, logger: DebugLogger, session: Optional[\"requests.Session\"]=None) -> Optional[Dict[str,Any]]:\n",
        "    if cik in _SEC_CACHE:\n",
        "        return _SEC_CACHE[cik]\n",
        "    if requests is None:\n",
        "        logger.log(\"requests indisponível p/ SEC.\")\n",
        "        return None\n",
        "    try:\n",
        "        # tentativa de cache em disco\n",
        "        cache_fp = _sec_cache_path(cik)\n",
        "        if cache_fp and cache_fp.exists():\n",
        "            try:\n",
        "                js = json.loads(cache_fp.read_text(encoding=\"utf-8\"))\n",
        "                _SEC_CACHE[cik] = js\n",
        "                logger.log(f\"SEC companyfacts (cache hit disco): CIK={cik}\")\n",
        "                return js\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        s = session or requests.Session()\n",
        "        r = s.get(SEC_FACTS_URL.format(CIK=cik), headers=_sec_headers(), timeout=60)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "        _SEC_CACHE[cik] = js\n",
        "        if cache_fp:\n",
        "            try:\n",
        "                cache_fp.write_text(json.dumps(js), encoding=\"utf-8\")\n",
        "            except Exception:\n",
        "                pass\n",
        "        logger.log(f\"SEC companyfacts baixado: CIK={cik}\")\n",
        "        return js\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Falha SEC companyfacts: {e}\")\n",
        "        return None\n",
        "\n",
        "def _pick_unit(units: Dict[str, List[Dict[str,Any]]]) -> Optional[str]:\n",
        "    if not units:\n",
        "        return None\n",
        "    for k in UNIT_PREF:\n",
        "        if k in units:\n",
        "            return k\n",
        "    # qualquer um\n",
        "    return next(iter(units.keys()), None)\n",
        "\n",
        "def extract_filings_from_facts(js: Dict[str,Any],\n",
        "                               logger: DebugLogger,\n",
        "                               canon_map: Dict[str, Dict[str, List[str]]] = CANON_MAP\n",
        "                               ) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constrói um DF 'filings' com colunas:\n",
        "      ['kpi','tax','tag','unit','end','filed','val','form']\n",
        "    Escolhe, para cada KPI canônica, o MELHOR tag disponível (maior número de pontos).\n",
        "    \"\"\"\n",
        "    if not js:\n",
        "        return pd.DataFrame(columns=[\"kpi\",\"tax\",\"tag\",\"unit\",\"end\",\"filed\",\"val\",\"form\"])\n",
        "    facts = js.get(\"facts\", {})\n",
        "    rows: List[Dict[str,Any]] = []\n",
        "\n",
        "    def collect_records(tax_name: str, tag_name: str) -> List[Dict[str,Any]]:\n",
        "        tax = facts.get(tax_name, {})\n",
        "        node = tax.get(tag_name)\n",
        "        if not node:\n",
        "            return []\n",
        "        units = node.get(\"units\", {})\n",
        "        unit_key = _pick_unit(units)\n",
        "        if not unit_key:\n",
        "            return []\n",
        "        out = []\n",
        "        for rec in units.get(unit_key, []):\n",
        "            form = rec.get(\"form\")\n",
        "            if form not in FORM_OK:\n",
        "                continue\n",
        "            end = pd.to_datetime(rec.get(\"end\"), errors=\"coerce\", utc=True)\n",
        "            filed = pd.to_datetime(rec.get(\"filed\"), errors=\"coerce\", utc=True)\n",
        "            val = pd.to_numeric(rec.get(\"val\"), errors=\"coerce\")\n",
        "            if pd.isna(end) or pd.isna(filed) or pd.isna(val):\n",
        "                continue\n",
        "            out.append({\"tax\": tax_name, \"tag\": tag_name, \"unit\": unit_key,\n",
        "                        \"end\": end, \"filed\": filed, \"val\": float(val), \"form\": form})\n",
        "        return out\n",
        "\n",
        "    # buscar candidatos e escolher o melhor por KPI canônica\n",
        "    for kpi, m in canon_map.items():\n",
        "        candidates: List[Dict[str, Any]] = []\n",
        "\n",
        "        for tax_name, tag_list in m.items():\n",
        "            for tag in tag_list:\n",
        "                recs = collect_records(tax_name, tag)\n",
        "                if recs:\n",
        "                    for r in recs:\n",
        "                        r[\"kpi\"] = kpi\n",
        "                    candidates.extend(recs)\n",
        "\n",
        "        if not candidates:\n",
        "            # nenhum tag disponível para esta KPI -> não cria linha alguma\n",
        "            continue\n",
        "\n",
        "        # preferir aquele tag/tax com MAIS pontos\n",
        "        df_cand = pd.DataFrame(candidates)\n",
        "        # qual (tax,tag,unit) com mais registros?\n",
        "        grp = df_cand.groupby([\"tax\",\"tag\",\"unit\"], as_index=False).size().sort_values(\"size\", ascending=False)\n",
        "        best_tax, best_tag, best_unit = grp.iloc[0][[\"tax\",\"tag\",\"unit\"]]\n",
        "\n",
        "        df_best = df_cand[(df_cand[\"tax\"]==best_tax)&(df_cand[\"tag\"]==best_tag)&(df_cand[\"unit\"]==best_unit)] \\\n",
        "                    .sort_values([\"filed\",\"end\"])\n",
        "        rows.extend(df_best.to_dict(\"records\"))\n",
        "\n",
        "        logger.log(f\"KPI '{kpi}': usando {best_tax}:{best_tag} [{best_unit}], pontos={len(df_best)}\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(columns=[\"kpi\",\"tax\",\"tag\",\"unit\",\"end\",\"filed\",\"val\",\"form\"])\n",
        "    df = df.sort_values([\"kpi\",\"filed\",\"end\"]).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ===================== Calendário de pregão & lock-to-publication =====================\n",
        "def trading_days_from_price(price: pd.DataFrame) -> pd.DatetimeIndex:\n",
        "    ix = pd.DatetimeIndex(pd.to_datetime(price[\"date\"], utc=True).dropna().unique())\n",
        "    return ix.sort_values()\n",
        "\n",
        "def align_to_next_trading_day(dt: pd.Timestamp, trade_days: pd.DatetimeIndex) -> Optional[pd.Timestamp]:\n",
        "    pos = trade_days.searchsorted(dt)\n",
        "    if pos < len(trade_days):\n",
        "        return trade_days[pos]\n",
        "    return None\n",
        "\n",
        "def kpis_step_to_trading_days(filings: pd.DataFrame,\n",
        "                              trade_days: pd.DatetimeIndex,\n",
        "                              logger: DebugLogger) -> Tuple[pd.DataFrame, Dict[str, Set[pd.Timestamp]]]:\n",
        "    \"\"\"\n",
        "    Para cada KPI:\n",
        "      - marca 'report days' = primeiro pregão >= filed\n",
        "      - gera série diária com merge_asof (último filed <= date)\n",
        "      - não preenche nada antes do primeiro filed (NaN), não interpola.\n",
        "    Retorna:\n",
        "      df_daily_step: ['date', <kpi...>]\n",
        "      report_days_map: dict(kpi -> set(dates))\n",
        "    \"\"\"\n",
        "    out = pd.DataFrame({\"date\": trade_days})\n",
        "    report_days: Dict[str, Set[pd.Timestamp]] = {}\n",
        "    if filings.empty:\n",
        "        return out, report_days\n",
        "\n",
        "    for kpi, grp in filings.groupby(\"kpi\"):\n",
        "        g = grp[[\"filed\",\"val\"]].dropna().sort_values(\"filed\").copy()\n",
        "        g[\"filed\"] = pd.to_datetime(g[\"filed\"], utc=True)\n",
        "\n",
        "        # report days (alinhados ao próximo pregão)\n",
        "        rset: Set[pd.Timestamp] = set()\n",
        "        for d in g[\"filed\"]:\n",
        "            nd = align_to_next_trading_day(d, trade_days)\n",
        "            if nd is not None:\n",
        "                rset.add(nd)\n",
        "        report_days[kpi] = rset\n",
        "\n",
        "        # merge_asof (último filed <= date)\n",
        "        merged = pd.merge_asof(\n",
        "            out[[\"date\"]].sort_values(\"date\"),\n",
        "            g.rename(columns={\"filed\":\"key\"}).sort_values(\"key\"),\n",
        "            left_on=\"date\", right_on=\"key\", direction=\"backward\"\n",
        "        )\n",
        "        out[kpi] = merged[\"val\"].values  # sem preenchimento antes do 1º filed -> NaN\n",
        "\n",
        "    return out, report_days\n",
        "\n",
        "\n",
        "# ===================== QA simples: saltos de preço =====================\n",
        "def qc_jumps_flag(df: pd.DataFrame, logger: DebugLogger,\n",
        "                  col_price: str = \"close_adjusted\",\n",
        "                  min_thr_abs: float = 0.6, mad_mult: float = 6.0) -> pd.DataFrame:\n",
        "    if df.empty or col_price not in df.columns:\n",
        "        return df\n",
        "    s = df[col_price].astype(float)\n",
        "    logret = np.log(s) - np.log(s.shift(1))\n",
        "    med = np.nanmedian(logret.values)\n",
        "    mad = np.nanmedian(np.abs(logret.values - med)) * 1.4826\n",
        "    thr = max(min_thr_abs, mad_mult * (mad if mad > 0 else 1e-6))\n",
        "    flag = (np.abs(logret) > thr)\n",
        "    df[\"flag_jump\"] = flag.fillna(False).astype(bool)\n",
        "    logger.log(\"QA preço (logret/MAD) aplicado\", jumps=int(df[\"flag_jump\"].sum()), thr=float(thr))\n",
        "    return df\n",
        "\n",
        "\n",
        "# ===================== JSON builder =====================\n",
        "def _checksum_bytes(b: bytes) -> str:\n",
        "    return hashlib.md5(b).hexdigest()\n",
        "\n",
        "def build_json_daily(entrada: Dict[str,Any],\n",
        "                     df_prices: pd.DataFrame,\n",
        "                     df_final: pd.DataFrame,\n",
        "                     kpis_kept: List[str],\n",
        "                     logger: DebugLogger) -> Dict[str,Any]:\n",
        "    ticker = (entrada.get(\"codigo_da_ação\") or entrada.get(\"ticker\") or entrada.get(\"codigo_da_acao\") or \"\").upper()\n",
        "    nome = entrada.get(\"Nome_da_empresa\", \"\")\n",
        "    dmin = pd.to_datetime(df_final[\"date\"]).min()\n",
        "    dmax = pd.to_datetime(df_final[\"date\"]).max()\n",
        "\n",
        "    # checksums\n",
        "    try:\n",
        "        b1 = df_prices.to_csv(index=False).encode()\n",
        "        b2 = df_final.to_csv(index=False).encode()\n",
        "        c1 = _checksum_bytes(b1); c2 = _checksum_bytes(b2)\n",
        "    except Exception:\n",
        "        c1 = c2 = None\n",
        "\n",
        "    johnson = {\n",
        "        \"meta_input\": {\n",
        "            \"timezone\": \"UTC\",\n",
        "            \"trading_calendar\": \"NYSE\",\n",
        "            \"index_type\": \"trading_days_NYSE\",\n",
        "            \"ajuste_preco\": \"adjusted_close_dividends_splits\",\n",
        "        },\n",
        "        \"empresa\": {\n",
        "            \"ticker\": ticker,\n",
        "            \"nome\": nome,\n",
        "            \"setor\": entrada.get(\"Setor_da_empresa\"),\n",
        "        },\n",
        "        \"alvo\": {\n",
        "            \"variavel\": \"preco_acao\",\n",
        "            \"tipo_alvo\": \"preco\",\n",
        "            \"unidade\": \"USD\",\n",
        "            \"frequencia\": \"diaria\",\n",
        "            \"coluna_valor\": \"close_adjusted\",\n",
        "        },\n",
        "        \"variaveis\": {\n",
        "            \"kpis\": [{\"nome\": t, \"frequencia\": \"diaria (lock_to_publication)\", \"fonte\": \"SEC companyfacts\"} for t in kpis_kept]\n",
        "        },\n",
        "        \"operacao\": {\n",
        "            \"coleta_em\": datetime.now(timezone.utc).isoformat(timespec='seconds'),\n",
        "            \"datas\": {\"min\": str(dmin.date()), \"max\": str(dmax.date())},\n",
        "            \"linhas\": int(len(df_final)),\n",
        "            \"colunas\": list(map(str, df_final.columns)),\n",
        "        },\n",
        "        \"qa_flags\": {\n",
        "            \"anomalias_jump\": int(df_final.get(\"flag_jump\", pd.Series(False)).sum())\n",
        "        },\n",
        "        \"proveniencia_opcional\": {\n",
        "            \"checksum_prices_csv_md5\": c1,\n",
        "            \"checksum_dataset_csv_md5\": c2,\n",
        "        }\n",
        "    }\n",
        "    return johnson\n",
        "\n",
        "\n",
        "# ===================== Função principal =====================\n",
        "def pesquisa_acao_daily_kpis(entrada: Dict[str,Any], *,\n",
        "                             debug: bool=True,\n",
        "                             sources: str=\"yfinance_only\",   # \"yfinance_only\" | \"all\"\n",
        "                             use_alphavantage: bool=False,\n",
        "                             compute_disparity: bool=False,\n",
        "                             drop_empty_kpi_cols: bool=True,\n",
        "                             years_back: Optional[int]=None   # None = 'max'\n",
        "                             ) -> Tuple[Dict[str,Any], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Retorna (json, df_diario_completo) — diário, sem imputação, com KPIs corretas (US-GAAP/IFRS).\n",
        "\n",
        "    Parâmetros chave:\n",
        "      - sources=\"yfinance_only\" (rápido) | \"all\" (tenta Stooq e AlphaVantage se ativado).\n",
        "      - use_alphavantage=True só se tiver ALPHAVANTAGE_API_KEY.\n",
        "      - compute_disparity=True calcula disparidade entre fontes (se 'all').\n",
        "      - drop_empty_kpi_cols=True remove KPI 100% NaN (e flags correspondentes).\n",
        "      - years_back: limite de anos para preço (ex.: 20). None = tudo (default yfinance 'max').\n",
        "    \"\"\"\n",
        "    logger = DebugLogger(enabled=debug)\n",
        "    logger.section(\"Início pesquisa_acao_daily_kpis\")\n",
        "\n",
        "    ticker = (entrada.get(\"codigo_da_ação\") or entrada.get(\"ticker\") or entrada.get(\"codigo_da_acao\") or \"\").upper()\n",
        "    if not ticker:\n",
        "        raise ValueError(\"Entrada precisa de 'codigo_da_ação' ou 'ticker'.\")\n",
        "    logger.log(f\"Ticker={ticker}, Empresa={entrada.get('Nome_da_empresa','')}\")\n",
        "\n",
        "    # 1) PREÇO — coleta multi-fonte (conforme configuração)\n",
        "    logger.section(\"Coleta de preço — fontes grátis\")\n",
        "    dfs_price: List[pd.DataFrame] = []\n",
        "    yf_period = \"max\" if years_back is None else f\"{years_back}y\"\n",
        "    yfd = fetch_prices_yfinance(ticker, logger, period=yf_period)\n",
        "    if yfd is not None:\n",
        "        dfs_price.append(yfd)\n",
        "\n",
        "    if sources == \"all\":\n",
        "        stq = fetch_prices_stooq(ticker, logger)\n",
        "        if stq is not None: dfs_price.append(stq)\n",
        "        if use_alphavantage:\n",
        "            av = fetch_prices_alphavantage(ticker, logger)\n",
        "            if av is not None: dfs_price.append(av)\n",
        "\n",
        "    if not dfs_price:\n",
        "        raise RuntimeError(\"Falha ao obter preço de todas as fontes.\")\n",
        "    for i, d in enumerate(dfs_price):\n",
        "        logger.df(f\"preco_src_{i+1}\", d, head=3)\n",
        "\n",
        "    logger.section(\"Stitching de preço\")\n",
        "    price = stitch_price_sources(dfs_price, logger, compute_disparity=(compute_disparity and len(dfs_price)>1))\n",
        "    price[\"ticker\"] = ticker\n",
        "    logger.df(\"df_prices_stitched\", price, head=6)\n",
        "\n",
        "    trade_days = trading_days_from_price(price)\n",
        "\n",
        "    # 2) SEC — KPIs (US-GAAP + IFRS) com escolha de melhor tag por KPI canônica\n",
        "    logger.section(\"Fundamentos (SEC) lock-to-publication (sem imputação)\")\n",
        "    session = requests.Session() if requests is not None else None\n",
        "    cik = resolve_cik_from_ticker(ticker, logger, session=session)\n",
        "    if cik:\n",
        "        facts_js = fetch_companyfacts_json(cik, logger, session=session)\n",
        "        filings = extract_filings_from_facts(facts_js, logger, CANON_MAP)\n",
        "    else:\n",
        "        filings = pd.DataFrame(columns=[\"kpi\",\"tax\",\"tag\",\"unit\",\"end\",\"filed\",\"val\",\"form\"])\n",
        "    logger.df(\"sec_filings (selecionadas)\", filings, head=8)\n",
        "\n",
        "    # 3) KPI diário stepwise (lock-to-publication) + flags report\n",
        "    df_kpis_daily, report_days = kpis_step_to_trading_days(filings, trade_days, logger)\n",
        "    logger.df(\"kpis_daily_step (raw)\", df_kpis_daily, head=8)\n",
        "\n",
        "    # 4) Consolidar com PREÇO\n",
        "    df = price.merge(df_kpis_daily, on=\"date\", how=\"left\")\n",
        "\n",
        "    # 5) Flags 'is_report_*' (booleanas) sem NaN\n",
        "    for kpi, rset in report_days.items():\n",
        "        mask = df[\"date\"].isin(list(rset))\n",
        "        df[f\"is_report_{kpi}\"] = mask.fillna(False).astype(bool)\n",
        "\n",
        "    # 6) QA simples de preço\n",
        "    df = qc_jumps_flag(df, logger)\n",
        "\n",
        "    # 7) Remover KPIs 100% vazias (e suas flags)\n",
        "    if drop_empty_kpi_cols:\n",
        "        kpi_cols = [c for c in df.columns if c not in {\n",
        "            \"date\",\"ticker\",\"close_adjusted\",\"Dividends\",\"Stock Splits\",\"n_sources\",\"disparity_bps\",\"flag_disparity\",\"flag_jump\"\n",
        "        } and not c.startswith(\"is_report_\")]\n",
        "        to_drop: List[str] = []\n",
        "        kept: List[str] = []\n",
        "        for c in kpi_cols:\n",
        "            if df[c].notna().sum() == 0:\n",
        "                to_drop.append(c)\n",
        "                flag_col = f\"is_report_{c}\"\n",
        "                if flag_col in df.columns:\n",
        "                    to_drop.append(flag_col)\n",
        "            else:\n",
        "                kept.append(c)\n",
        "        if to_drop:\n",
        "            df = df.drop(columns=[c for c in to_drop if c in df.columns])\n",
        "            logger.log(f\"KPIs removidas (100% vazias): {sorted(set(to_drop))}\")\n",
        "        logger.log(f\"KPIs mantidas: {kept}\")\n",
        "        kpis_kept = kept\n",
        "    else:\n",
        "        kpis_kept = [c for c in df.columns if c not in {\n",
        "            \"date\",\"ticker\",\"close_adjusted\",\"Dividends\",\"Stock Splits\",\"n_sources\",\"disparity_bps\",\"flag_disparity\",\"flag_jump\"\n",
        "        } and not c.startswith(\"is_report_\")]\n",
        "\n",
        "    # 8) Ordenar colunas\n",
        "    base_cols = [\"date\",\"ticker\",\"close_adjusted\",\"Dividends\",\"Stock Splits\",\"n_sources\",\"disparity_bps\",\"flag_disparity\",\"flag_jump\"]\n",
        "    ordered = [c for c in base_cols if c in df.columns] + sorted(kpis_kept) + [f\"is_report_{k}\" for k in sorted(kpis_kept)]\n",
        "    df = df[ordered]\n",
        "    logger.df(\"df_final (amostra)\", df, head=12)\n",
        "\n",
        "    # 9) JSON\n",
        "    johnson = build_json_daily(entrada, price, df, kpis_kept, logger)\n",
        "\n",
        "    # 10) Resumo\n",
        "    logger.section(\"Resumo\")\n",
        "    logger.log(f\"Período do dataset: {df['date'].min().date()} .. {df['date'].max().date()}\")\n",
        "    logger.log(f\"Linhas: {len(df)}, Colunas: {len(df.columns)}\")\n",
        "\n",
        "    return johnson, df\n",
        "\n",
        "\n",
        "# ---------------- Execução direta (exemplo) ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    exemplo = {\n",
        "        \"codigo_da_ação\": \"PBR\",  # Petrobras ADR\n",
        "        \"Nome_da_empresa\": \"Petroleo Brasileiro S.A. - Petrobras\",\n",
        "        \"Setor_da_empresa\": \"Energy\",\n",
        "    }\n",
        "    johnson, df = pesquisa_acao_daily_kpis(\n",
        "        exemplo,\n",
        "        debug=True,\n",
        "        sources=\"yfinance_only\",   # troque para \"all\" se quiser Stooq/AV\n",
        "        use_alphavantage=False,\n",
        "        compute_disparity=False,\n",
        "        drop_empty_kpi_cols=True,\n",
        "        years_back=None,           # None = todo histórico do yfinance\n",
        "    )\n",
        "\n",
        "    print(\"\\n===== JSON (chaves principais) =====\")\n",
        "    print({k: johnson[k] for k in [\"empresa\",\"variaveis\",\"operacao\",\"qa_flags\"]})\n",
        "\n",
        "    print(\"\\n===== PRIMEIRAS 20 LINHAS =====\")\n",
        "    with pd.option_context('display.max_columns', None, 'display.width', 240):\n",
        "        print(df.head(20))"
      ],
      "metadata": {
        "id": "MBQSnbE8duC1",
        "outputId": "ea0a80db-3d8e-4139-a892-6c2583e48ac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Início pesquisa_acao_daily_kpis\n",
            "================================================================================\n",
            "[DEBUG] Ticker=PBR, Empresa=Petroleo Brasileiro S.A. - Petrobras\n",
            "\n",
            "================================================================================\n",
            "Coleta de preço — fontes grátis\n",
            "================================================================================\n",
            "[DEBUG] yfinance OK: PBR, linhas=6309\n",
            "[DEBUG][DF] preco_src_1: shape=(6309, 5), cols=['date', 'close_adjusted', 'Dividends', 'Stock Splits', 'source']\n",
            "                       date  close_adjusted  Dividends  Stock Splits    source\n",
            "0 2000-08-10 04:00:00+00:00        1.280023        0.0           0.0  yfinance\n",
            "1 2000-08-11 04:00:00+00:00        1.277228        0.0           0.0  yfinance\n",
            "2 2000-08-14 04:00:00+00:00        1.271638        0.0           0.0  yfinance\n",
            "\n",
            "================================================================================\n",
            "Stitching de preço\n",
            "================================================================================\n",
            "[DEBUG] Stitching concluído\n",
            "[DEBUG][DF] df_prices_stitched: shape=(6309, 8), cols=['date', 'close_adjusted', 'n_sources', 'disparity_bps', 'flag_disparity', 'Dividends', 'Stock Splits', 'ticker']\n",
            "                       date  close_adjusted  n_sources  disparity_bps  flag_disparity  Dividends  Stock Splits ticker\n",
            "0 2000-08-10 04:00:00+00:00        1.280023          1            0.0           False        0.0           0.0    PBR\n",
            "1 2000-08-11 04:00:00+00:00        1.277228          1            0.0           False        0.0           0.0    PBR\n",
            "2 2000-08-14 04:00:00+00:00        1.271638          1            0.0           False        0.0           0.0    PBR\n",
            "3 2000-08-15 04:00:00+00:00        1.277228          1            0.0           False        0.0           0.0    PBR\n",
            "4 2000-08-16 04:00:00+00:00        1.310766          1            0.0           False        0.0           0.0    PBR\n",
            "5 2000-08-17 04:00:00+00:00        1.375046          1            0.0           False        0.0           0.0    PBR\n",
            "\n",
            "================================================================================\n",
            "Fundamentos (SEC) lock-to-publication (sem imputação)\n",
            "================================================================================\n",
            "[DEBUG] CIK resolvido: PBR->0001119639\n",
            "[DEBUG] SEC companyfacts baixado: CIK=0001119639\n",
            "[DEBUG] KPI 'Revenues': usando ifrs-full:Revenue [USD], pontos=31\n",
            "[DEBUG] KPI 'NetIncomeLoss': usando ifrs-full:ProfitLoss [USD], pontos=31\n",
            "[DEBUG] KPI 'OperatingIncomeLoss': usando us-gaap:OperatingIncomeLoss [USD], pontos=17\n",
            "[DEBUG] KPI 'GrossProfit': usando ifrs-full:GrossProfit [USD], pontos=31\n",
            "[DEBUG] KPI 'EarningsPerShareBasic': usando ifrs-full:BasicEarningsLossPerShare [USD/shares], pontos=14\n",
            "[DEBUG] KPI 'EarningsPerShareDiluted': usando ifrs-full:DilutedEarningsLossPerShare [USD/shares], pontos=6\n",
            "[DEBUG] KPI 'Assets': usando ifrs-full:Assets [USD], pontos=21\n",
            "[DEBUG] KPI 'Liabilities': usando ifrs-full:Liabilities [USD], pontos=20\n",
            "[DEBUG] KPI 'StockholdersEquity': usando ifrs-full:Equity [USD], pontos=40\n",
            "[DEBUG] KPI 'CashAndCashEquivalentsAtCarryingValue': usando us-gaap:CashAndCashEquivalentsAtCarryingValue [USD], pontos=30\n",
            "[DEBUG] KPI 'LongTermDebtNoncurrent': usando us-gaap:LongTermDebtNoncurrent [USD], pontos=14\n",
            "[DEBUG] KPI 'CurrentAssets': usando ifrs-full:CurrentAssets [USD], pontos=21\n",
            "[DEBUG] KPI 'CurrentLiabilities': usando ifrs-full:CurrentLiabilities [USD], pontos=20\n",
            "[DEBUG] KPI 'ResearchAndDevelopmentExpense': usando ifrs-full:ResearchAndDevelopmentExpense [USD], pontos=31\n",
            "[DEBUG] KPI 'CapitalExpenditures': usando us-gaap:PaymentsToAcquirePropertyPlantAndEquipment [USD], pontos=17\n",
            "[DEBUG][DF] sec_filings (selecionadas): shape=(344, 8), cols=['tax', 'tag', 'unit', 'end', 'filed', 'val', 'form', 'kpi']\n",
            "         tax     tag unit                       end                     filed           val  form     kpi\n",
            "0  ifrs-full  Assets  USD 2016-12-31 00:00:00+00:00 2018-04-18 00:00:00+00:00  2.469830e+11  20-F  Assets\n",
            "1  ifrs-full  Assets  USD 2017-12-31 00:00:00+00:00 2018-04-18 00:00:00+00:00  2.513660e+11  20-F  Assets\n",
            "2  ifrs-full  Assets  USD 2017-06-30 00:00:00+00:00 2018-08-20 00:00:00+00:00  2.513660e+11   6-K  Assets\n",
            "3  ifrs-full  Assets  USD 2017-12-31 00:00:00+00:00 2018-08-20 00:00:00+00:00  2.513660e+11   6-K  Assets\n",
            "4  ifrs-full  Assets  USD 2018-06-30 00:00:00+00:00 2018-08-20 00:00:00+00:00  2.205210e+11   6-K  Assets\n",
            "5  ifrs-full  Assets  USD 2017-12-31 00:00:00+00:00 2019-04-01 00:00:00+00:00  2.513660e+11  20-F  Assets\n",
            "6  ifrs-full  Assets  USD 2018-12-31 00:00:00+00:00 2019-04-01 00:00:00+00:00  2.220680e+11  20-F  Assets\n",
            "7  ifrs-full  Assets  USD 2017-12-31 00:00:00+00:00 2020-01-02 00:00:00+00:00  2.513660e+11   6-K  Assets\n",
            "[DEBUG][DF] kpis_daily_step (raw): shape=(6309, 16), cols=['date', 'Assets', 'CapitalExpenditures', 'CashAndCashEquivalentsAtCarryingValue', 'CurrentAssets', 'CurrentLiabilities', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'Liabilities', 'LongTermDebtNoncurrent', 'NetIncomeLoss', 'OperatingIncomeLoss', 'ResearchAndDevelopmentExpense', 'Revenues', 'StockholdersEquity']\n",
            "                       date  Assets  CapitalExpenditures  CashAndCashEquivalentsAtCarryingValue  CurrentAssets  CurrentLiabilities  EarningsPerShareBasic  \\\n",
            "0 2000-08-10 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "1 2000-08-11 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "2 2000-08-14 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "3 2000-08-15 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "4 2000-08-16 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "5 2000-08-17 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "6 2000-08-18 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "7 2000-08-21 04:00:00+00:00     NaN                  NaN                                    NaN            NaN                 NaN                    NaN   \n",
            "\n",
            "   EarningsPerShareDiluted  GrossProfit  Liabilities  LongTermDebtNoncurrent  NetIncomeLoss  OperatingIncomeLoss  ResearchAndDevelopmentExpense  Revenues  StockholdersEquity  \n",
            "0                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "1                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "2                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "3                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "4                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "5                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "6                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "7                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN  \n",
            "[DEBUG] QA preço (logret/MAD) aplicado\n",
            "[DEBUG] KPIs mantidas: ['Assets', 'CapitalExpenditures', 'CashAndCashEquivalentsAtCarryingValue', 'CurrentAssets', 'CurrentLiabilities', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'Liabilities', 'LongTermDebtNoncurrent', 'NetIncomeLoss', 'OperatingIncomeLoss', 'ResearchAndDevelopmentExpense', 'Revenues', 'StockholdersEquity']\n",
            "[DEBUG][DF] df_final (amostra): shape=(6309, 39), cols=['date', 'ticker', 'close_adjusted', 'Dividends', 'Stock Splits', 'n_sources', 'disparity_bps', 'flag_disparity', 'flag_jump', 'Assets', 'CapitalExpenditures', 'CashAndCashEquivalentsAtCarryingValue', 'CurrentAssets', 'CurrentLiabilities', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'Liabilities', 'LongTermDebtNoncurrent', 'NetIncomeLoss', 'OperatingIncomeLoss', 'ResearchAndDevelopmentExpense', 'Revenues', 'StockholdersEquity', 'is_report_Assets', 'is_report_CapitalExpenditures', 'is_report_CashAndCashEquivalentsAtCarryingValue', 'is_report_CurrentAssets', 'is_report_CurrentLiabilities', 'is_report_EarningsPerShareBasic', 'is_report_EarningsPerShareDiluted', 'is_report_GrossProfit', 'is_report_Liabilities', 'is_report_LongTermDebtNoncurrent', 'is_report_NetIncomeLoss', 'is_report_OperatingIncomeLoss', 'is_report_ResearchAndDevelopmentExpense', 'is_report_Revenues', 'is_report_StockholdersEquity']\n",
            "                        date ticker  close_adjusted  Dividends  Stock Splits  n_sources  disparity_bps  flag_disparity  flag_jump  Assets  CapitalExpenditures  \\\n",
            "0  2000-08-10 04:00:00+00:00    PBR        1.280023        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "1  2000-08-11 04:00:00+00:00    PBR        1.277228        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "2  2000-08-14 04:00:00+00:00    PBR        1.271638        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "3  2000-08-15 04:00:00+00:00    PBR        1.277228        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "4  2000-08-16 04:00:00+00:00    PBR        1.310766        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "5  2000-08-17 04:00:00+00:00    PBR        1.375046        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "6  2000-08-18 04:00:00+00:00    PBR        1.296792        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "7  2000-08-21 04:00:00+00:00    PBR        1.302381        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "8  2000-08-22 04:00:00+00:00    PBR        1.316355        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "9  2000-08-23 04:00:00+00:00    PBR        1.347098        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "10 2000-08-24 04:00:00+00:00    PBR        1.319150        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "11 2000-08-25 04:00:00+00:00    PBR        1.352688        0.0           0.0          1            0.0           False      False     NaN                  NaN   \n",
            "\n",
            "    CashAndCashEquivalentsAtCarryingValue  CurrentAssets  CurrentLiabilities  EarningsPerShareBasic  EarningsPerShareDiluted  GrossProfit  Liabilities  LongTermDebtNoncurrent  \\\n",
            "0                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "1                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "2                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "3                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "4                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "5                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "6                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "7                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "8                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "9                                     NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "10                                    NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "11                                    NaN            NaN                 NaN                    NaN                      NaN          NaN          NaN                     NaN   \n",
            "\n",
            "    NetIncomeLoss  OperatingIncomeLoss  ResearchAndDevelopmentExpense  Revenues  StockholdersEquity  is_report_Assets  is_report_CapitalExpenditures  \\\n",
            "0             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "1             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "2             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "3             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "4             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "5             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "6             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "7             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "8             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "9             NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "10            NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "11            NaN                  NaN                            NaN       NaN                 NaN             False                          False   \n",
            "\n",
            "    is_report_CashAndCashEquivalentsAtCarryingValue  is_report_CurrentAssets  is_report_CurrentLiabilities  is_report_EarningsPerShareBasic  is_report_EarningsPerShareDiluted  \\\n",
            "0                                             False                    False                         False                            False                              False   \n",
            "1                                             False                    False                         False                            False                              False   \n",
            "2                                             False                    False                         False                            False                              False   \n",
            "3                                             False                    False                         False                            False                              False   \n",
            "4                                             False                    False                         False                            False                              False   \n",
            "5                                             False                    False                         False                            False                              False   \n",
            "6                                             False                    False                         False                            False                              False   \n",
            "7                                             False                    False                         False                            False                              False   \n",
            "8                                             False                    False                         False                            False                              False   \n",
            "9                                             False                    False                         False                            False                              False   \n",
            "10                                            False                    False                         False                            False                              False   \n",
            "11                                            False                    False                         False                            False                              False   \n",
            "\n",
            "    is_report_GrossProfit  is_report_Liabilities  is_report_LongTermDebtNoncurrent  is_report_NetIncomeLoss  is_report_OperatingIncomeLoss  \\\n",
            "0                   False                  False                             False                    False                          False   \n",
            "1                   False                  False                             False                    False                          False   \n",
            "2                   False                  False                             False                    False                          False   \n",
            "3                   False                  False                             False                    False                          False   \n",
            "4                   False                  False                             False                    False                          False   \n",
            "5                   False                  False                             False                    False                          False   \n",
            "6                   False                  False                             False                    False                          False   \n",
            "7                   False                  False                             False                    False                          False   \n",
            "8                   False                  False                             False                    False                          False   \n",
            "9                   False                  False                             False                    False                          False   \n",
            "10                  False                  False                             False                    False                          False   \n",
            "11                  False                  False                             False                    False                          False   \n",
            "\n",
            "    is_report_ResearchAndDevelopmentExpense  is_report_Revenues  is_report_StockholdersEquity  \n",
            "0                                     False               False                         False  \n",
            "1                                     False               False                         False  \n",
            "2                                     False               False                         False  \n",
            "3                                     False               False                         False  \n",
            "4                                     False               False                         False  \n",
            "5                                     False               False                         False  \n",
            "6                                     False               False                         False  \n",
            "7                                     False               False                         False  \n",
            "8                                     False               False                         False  \n",
            "9                                     False               False                         False  \n",
            "10                                    False               False                         False  \n",
            "11                                    False               False                         False  \n",
            "\n",
            "================================================================================\n",
            "Resumo\n",
            "================================================================================\n",
            "[DEBUG] Período do dataset: 2000-08-10 .. 2025-09-11\n",
            "[DEBUG] Linhas: 6309, Colunas: 39\n",
            "\n",
            "===== JSON (chaves principais) =====\n",
            "{'empresa': {'ticker': 'PBR', 'nome': 'Petroleo Brasileiro S.A. - Petrobras', 'setor': 'Energy'}, 'variaveis': {'kpis': [{'nome': 'Assets', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CapitalExpenditures', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CashAndCashEquivalentsAtCarryingValue', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CurrentAssets', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CurrentLiabilities', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'EarningsPerShareBasic', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'EarningsPerShareDiluted', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'GrossProfit', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'Liabilities', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'LongTermDebtNoncurrent', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'NetIncomeLoss', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'OperatingIncomeLoss', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'ResearchAndDevelopmentExpense', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'Revenues', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'StockholdersEquity', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}]}, 'operacao': {'coleta_em': '2025-09-12T00:10:53+00:00', 'datas': {'min': '2000-08-10', 'max': '2025-09-11'}, 'linhas': 6309, 'colunas': ['date', 'ticker', 'close_adjusted', 'Dividends', 'Stock Splits', 'n_sources', 'disparity_bps', 'flag_disparity', 'flag_jump', 'Assets', 'CapitalExpenditures', 'CashAndCashEquivalentsAtCarryingValue', 'CurrentAssets', 'CurrentLiabilities', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'Liabilities', 'LongTermDebtNoncurrent', 'NetIncomeLoss', 'OperatingIncomeLoss', 'ResearchAndDevelopmentExpense', 'Revenues', 'StockholdersEquity', 'is_report_Assets', 'is_report_CapitalExpenditures', 'is_report_CashAndCashEquivalentsAtCarryingValue', 'is_report_CurrentAssets', 'is_report_CurrentLiabilities', 'is_report_EarningsPerShareBasic', 'is_report_EarningsPerShareDiluted', 'is_report_GrossProfit', 'is_report_Liabilities', 'is_report_LongTermDebtNoncurrent', 'is_report_NetIncomeLoss', 'is_report_OperatingIncomeLoss', 'is_report_ResearchAndDevelopmentExpense', 'is_report_Revenues', 'is_report_StockholdersEquity']}, 'qa_flags': {'anomalias_jump': 0}}\n",
            "\n",
            "===== PRIMEIRAS 20 LINHAS =====\n",
            "                        date ticker  close_adjusted  Dividends  Stock Splits  n_sources  disparity_bps  flag_disparity  flag_jump  Assets  CapitalExpenditures  CashAndCashEquivalentsAtCarryingValue  CurrentAssets  CurrentLiabilities  \\\n",
            "0  2000-08-10 04:00:00+00:00    PBR        1.280023        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "1  2000-08-11 04:00:00+00:00    PBR        1.277228        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "2  2000-08-14 04:00:00+00:00    PBR        1.271638        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "3  2000-08-15 04:00:00+00:00    PBR        1.277228        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "4  2000-08-16 04:00:00+00:00    PBR        1.310766        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "5  2000-08-17 04:00:00+00:00    PBR        1.375046        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "6  2000-08-18 04:00:00+00:00    PBR        1.296792        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "7  2000-08-21 04:00:00+00:00    PBR        1.302381        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "8  2000-08-22 04:00:00+00:00    PBR        1.316355        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "9  2000-08-23 04:00:00+00:00    PBR        1.347098        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "10 2000-08-24 04:00:00+00:00    PBR        1.319150        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "11 2000-08-25 04:00:00+00:00    PBR        1.352688        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "12 2000-08-28 04:00:00+00:00    PBR        1.347098        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "13 2000-08-29 04:00:00+00:00    PBR        1.391815        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "14 2000-08-30 04:00:00+00:00    PBR        1.442122        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "15 2000-08-31 04:00:00+00:00    PBR        1.422558        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "16 2000-09-01 04:00:00+00:00    PBR        1.394611        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "17 2000-09-05 04:00:00+00:00    PBR        1.402995        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "18 2000-09-06 04:00:00+00:00    PBR        1.475660        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "19 2000-09-07 04:00:00+00:00    PBR        1.492428        0.0           0.0          1            0.0           False      False     NaN                  NaN                                    NaN            NaN                 NaN   \n",
            "\n",
            "    EarningsPerShareBasic  EarningsPerShareDiluted  GrossProfit  Liabilities  LongTermDebtNoncurrent  NetIncomeLoss  OperatingIncomeLoss  ResearchAndDevelopmentExpense  Revenues  StockholdersEquity  is_report_Assets  \\\n",
            "0                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "1                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "2                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "3                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "4                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "5                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "6                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "7                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "8                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "9                     NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "10                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "11                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "12                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "13                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "14                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "15                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "16                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "17                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "18                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "19                    NaN                      NaN          NaN          NaN                     NaN            NaN                  NaN                            NaN       NaN                 NaN             False   \n",
            "\n",
            "    is_report_CapitalExpenditures  is_report_CashAndCashEquivalentsAtCarryingValue  is_report_CurrentAssets  is_report_CurrentLiabilities  is_report_EarningsPerShareBasic  is_report_EarningsPerShareDiluted  is_report_GrossProfit  \\\n",
            "0                           False                                            False                    False                         False                            False                              False                  False   \n",
            "1                           False                                            False                    False                         False                            False                              False                  False   \n",
            "2                           False                                            False                    False                         False                            False                              False                  False   \n",
            "3                           False                                            False                    False                         False                            False                              False                  False   \n",
            "4                           False                                            False                    False                         False                            False                              False                  False   \n",
            "5                           False                                            False                    False                         False                            False                              False                  False   \n",
            "6                           False                                            False                    False                         False                            False                              False                  False   \n",
            "7                           False                                            False                    False                         False                            False                              False                  False   \n",
            "8                           False                                            False                    False                         False                            False                              False                  False   \n",
            "9                           False                                            False                    False                         False                            False                              False                  False   \n",
            "10                          False                                            False                    False                         False                            False                              False                  False   \n",
            "11                          False                                            False                    False                         False                            False                              False                  False   \n",
            "12                          False                                            False                    False                         False                            False                              False                  False   \n",
            "13                          False                                            False                    False                         False                            False                              False                  False   \n",
            "14                          False                                            False                    False                         False                            False                              False                  False   \n",
            "15                          False                                            False                    False                         False                            False                              False                  False   \n",
            "16                          False                                            False                    False                         False                            False                              False                  False   \n",
            "17                          False                                            False                    False                         False                            False                              False                  False   \n",
            "18                          False                                            False                    False                         False                            False                              False                  False   \n",
            "19                          False                                            False                    False                         False                            False                              False                  False   \n",
            "\n",
            "    is_report_Liabilities  is_report_LongTermDebtNoncurrent  is_report_NetIncomeLoss  is_report_OperatingIncomeLoss  is_report_ResearchAndDevelopmentExpense  is_report_Revenues  is_report_StockholdersEquity  \n",
            "0                   False                             False                    False                          False                                    False               False                         False  \n",
            "1                   False                             False                    False                          False                                    False               False                         False  \n",
            "2                   False                             False                    False                          False                                    False               False                         False  \n",
            "3                   False                             False                    False                          False                                    False               False                         False  \n",
            "4                   False                             False                    False                          False                                    False               False                         False  \n",
            "5                   False                             False                    False                          False                                    False               False                         False  \n",
            "6                   False                             False                    False                          False                                    False               False                         False  \n",
            "7                   False                             False                    False                          False                                    False               False                         False  \n",
            "8                   False                             False                    False                          False                                    False               False                         False  \n",
            "9                   False                             False                    False                          False                                    False               False                         False  \n",
            "10                  False                             False                    False                          False                                    False               False                         False  \n",
            "11                  False                             False                    False                          False                                    False               False                         False  \n",
            "12                  False                             False                    False                          False                                    False               False                         False  \n",
            "13                  False                             False                    False                          False                                    False               False                         False  \n",
            "14                  False                             False                    False                          False                                    False               False                         False  \n",
            "15                  False                             False                    False                          False                                    False               False                         False  \n",
            "16                  False                             False                    False                          False                                    False               False                         False  \n",
            "17                  False                             False                    False                          False                                    False               False                         False  \n",
            "18                  False                             False                    False                          False                                    False               False                         False  \n",
            "19                  False                             False                    False                          False                                    False               False                         False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "lt4YBC2ZZxks",
        "outputId": "98472eba-454e-42ea-d5c4-ffba5afdebfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       close_adjusted    Dividends  Stock Splits  n_sources  disparity_bps  \\\n",
              "count     6307.000000  6307.000000   6307.000000     6307.0         6307.0   \n",
              "mean         5.459530     0.003902      0.000634        1.0            0.0   \n",
              "std          3.914217     0.057248      0.035612        0.0            0.0   \n",
              "min          0.464958     0.000000      0.000000        1.0            0.0   \n",
              "25%          2.393611     0.000000      0.000000        1.0            0.0   \n",
              "50%          4.497064     0.000000      0.000000        1.0            0.0   \n",
              "75%          7.748088     0.000000      0.000000        1.0            0.0   \n",
              "max         18.641769     2.590000      2.000000        1.0            0.0   \n",
              "\n",
              "             Assets  CapitalExpenditures  \\\n",
              "count  1.859000e+03         3.850000e+03   \n",
              "mean   2.055890e+11         3.117989e+10   \n",
              "std    2.223272e+10         4.521345e+09   \n",
              "min    1.743480e+11         9.783000e+09   \n",
              "25%    1.871910e+11         3.178500e+10   \n",
              "50%    2.170670e+11         3.178500e+10   \n",
              "75%    2.220680e+11         3.178500e+10   \n",
              "max    2.513660e+11         4.507800e+10   \n",
              "\n",
              "       CashAndCashEquivalentsAtCarryingValue  CurrentAssets  \\\n",
              "count                           3.850000e+03   1.859000e+03   \n",
              "mean                            1.745076e+10   3.171624e+10   \n",
              "std                             1.096709e+09   5.317609e+09   \n",
              "min                             1.297200e+10   2.183600e+10   \n",
              "25%                             1.762400e+10   2.781200e+10   \n",
              "50%                             1.762400e+10   3.125000e+10   \n",
              "75%                             1.762400e+10   3.706200e+10   \n",
              "max                             2.168900e+10   4.713100e+10   \n",
              "\n",
              "       CurrentLiabilities  EarningsPerShareBasic  EarningsPerShareDiluted  \\\n",
              "count        1.859000e+03            1374.000000                353.00000   \n",
              "mean         2.777034e+10               1.485058                  1.49932   \n",
              "std          3.721474e+09               0.784664                  0.61532   \n",
              "min          2.195400e+10               0.580000                  0.58000   \n",
              "25%          2.494800e+10               0.780000                  0.58000   \n",
              "50%          2.622500e+10               1.520000                  1.91000   \n",
              "75%          3.138000e+10               1.910000                  1.91000   \n",
              "max          3.386000e+10               2.810000                  1.91000   \n",
              "\n",
              "        GrossProfit   Liabilities  LongTermDebtNoncurrent  NetIncomeLoss  \\\n",
              "count  1.859000e+03  1.859000e+03            3.850000e+03   1.859000e+03   \n",
              "mean   3.870652e+10  1.345753e+11            6.641099e+10   1.392833e+10   \n",
              "std    1.569530e+10  1.821358e+10            4.179657e+09   1.226226e+10   \n",
              "min    8.771000e+09  1.045360e+11            4.814900e+10   1.690000e+08   \n",
              "25%    2.868000e+10  1.173550e+11            6.752800e+10   2.688000e+09   \n",
              "50%    3.406700e+10  1.380920e+11            6.752800e+10   7.414000e+09   \n",
              "75%    5.397400e+10  1.488930e+11            6.752800e+10   2.499500e+10   \n",
              "max    6.498800e+10  1.698640e+11            7.052900e+10   3.675500e+10   \n",
              "\n",
              "       OperatingIncomeLoss  ResearchAndDevelopmentExpense      Revenues  \\\n",
              "count         3.850000e+03                   1.859000e+03  1.859000e+03   \n",
              "mean          2.122647e+10                   4.383873e+08  8.488665e+10   \n",
              "std           2.789686e+09                   4.354769e+08  2.685101e+10   \n",
              "min           6.209000e+09                  -5.630000e+08  2.340700e+10   \n",
              "25%           2.187400e+10                   3.550000e+08  8.396600e+10   \n",
              "50%           2.187400e+10                   6.410000e+08  8.463800e+10   \n",
              "75%           2.187400e+10                   7.260000e+08  1.024090e+11   \n",
              "max           2.415800e+10                   7.920000e+08  1.244740e+11   \n",
              "\n",
              "       StockholdersEquity  \n",
              "count        1.859000e+03  \n",
              "mean         7.199532e+10  \n",
              "std          7.274943e+09  \n",
              "min          5.935000e+10  \n",
              "25%          6.981200e+10  \n",
              "50%          7.421500e+10  \n",
              "75%          7.897500e+10  \n",
              "max          8.150200e+10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a502791d-3cde-43eb-8f35-7106bfc76359\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>close_adjusted</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Stock Splits</th>\n",
              "      <th>n_sources</th>\n",
              "      <th>disparity_bps</th>\n",
              "      <th>Assets</th>\n",
              "      <th>CapitalExpenditures</th>\n",
              "      <th>CashAndCashEquivalentsAtCarryingValue</th>\n",
              "      <th>CurrentAssets</th>\n",
              "      <th>CurrentLiabilities</th>\n",
              "      <th>EarningsPerShareBasic</th>\n",
              "      <th>EarningsPerShareDiluted</th>\n",
              "      <th>GrossProfit</th>\n",
              "      <th>Liabilities</th>\n",
              "      <th>LongTermDebtNoncurrent</th>\n",
              "      <th>NetIncomeLoss</th>\n",
              "      <th>OperatingIncomeLoss</th>\n",
              "      <th>ResearchAndDevelopmentExpense</th>\n",
              "      <th>Revenues</th>\n",
              "      <th>StockholdersEquity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6307.000000</td>\n",
              "      <td>6307.000000</td>\n",
              "      <td>6307.000000</td>\n",
              "      <td>6307.0</td>\n",
              "      <td>6307.0</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>3.850000e+03</td>\n",
              "      <td>3.850000e+03</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>1374.000000</td>\n",
              "      <td>353.00000</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>3.850000e+03</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>3.850000e+03</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>1.859000e+03</td>\n",
              "      <td>1.859000e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.459530</td>\n",
              "      <td>0.003902</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.055890e+11</td>\n",
              "      <td>3.117989e+10</td>\n",
              "      <td>1.745076e+10</td>\n",
              "      <td>3.171624e+10</td>\n",
              "      <td>2.777034e+10</td>\n",
              "      <td>1.485058</td>\n",
              "      <td>1.49932</td>\n",
              "      <td>3.870652e+10</td>\n",
              "      <td>1.345753e+11</td>\n",
              "      <td>6.641099e+10</td>\n",
              "      <td>1.392833e+10</td>\n",
              "      <td>2.122647e+10</td>\n",
              "      <td>4.383873e+08</td>\n",
              "      <td>8.488665e+10</td>\n",
              "      <td>7.199532e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.914217</td>\n",
              "      <td>0.057248</td>\n",
              "      <td>0.035612</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.223272e+10</td>\n",
              "      <td>4.521345e+09</td>\n",
              "      <td>1.096709e+09</td>\n",
              "      <td>5.317609e+09</td>\n",
              "      <td>3.721474e+09</td>\n",
              "      <td>0.784664</td>\n",
              "      <td>0.61532</td>\n",
              "      <td>1.569530e+10</td>\n",
              "      <td>1.821358e+10</td>\n",
              "      <td>4.179657e+09</td>\n",
              "      <td>1.226226e+10</td>\n",
              "      <td>2.789686e+09</td>\n",
              "      <td>4.354769e+08</td>\n",
              "      <td>2.685101e+10</td>\n",
              "      <td>7.274943e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.464958</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.743480e+11</td>\n",
              "      <td>9.783000e+09</td>\n",
              "      <td>1.297200e+10</td>\n",
              "      <td>2.183600e+10</td>\n",
              "      <td>2.195400e+10</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.58000</td>\n",
              "      <td>8.771000e+09</td>\n",
              "      <td>1.045360e+11</td>\n",
              "      <td>4.814900e+10</td>\n",
              "      <td>1.690000e+08</td>\n",
              "      <td>6.209000e+09</td>\n",
              "      <td>-5.630000e+08</td>\n",
              "      <td>2.340700e+10</td>\n",
              "      <td>5.935000e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.393611</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.871910e+11</td>\n",
              "      <td>3.178500e+10</td>\n",
              "      <td>1.762400e+10</td>\n",
              "      <td>2.781200e+10</td>\n",
              "      <td>2.494800e+10</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.58000</td>\n",
              "      <td>2.868000e+10</td>\n",
              "      <td>1.173550e+11</td>\n",
              "      <td>6.752800e+10</td>\n",
              "      <td>2.688000e+09</td>\n",
              "      <td>2.187400e+10</td>\n",
              "      <td>3.550000e+08</td>\n",
              "      <td>8.396600e+10</td>\n",
              "      <td>6.981200e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.497064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.170670e+11</td>\n",
              "      <td>3.178500e+10</td>\n",
              "      <td>1.762400e+10</td>\n",
              "      <td>3.125000e+10</td>\n",
              "      <td>2.622500e+10</td>\n",
              "      <td>1.520000</td>\n",
              "      <td>1.91000</td>\n",
              "      <td>3.406700e+10</td>\n",
              "      <td>1.380920e+11</td>\n",
              "      <td>6.752800e+10</td>\n",
              "      <td>7.414000e+09</td>\n",
              "      <td>2.187400e+10</td>\n",
              "      <td>6.410000e+08</td>\n",
              "      <td>8.463800e+10</td>\n",
              "      <td>7.421500e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.748088</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.220680e+11</td>\n",
              "      <td>3.178500e+10</td>\n",
              "      <td>1.762400e+10</td>\n",
              "      <td>3.706200e+10</td>\n",
              "      <td>3.138000e+10</td>\n",
              "      <td>1.910000</td>\n",
              "      <td>1.91000</td>\n",
              "      <td>5.397400e+10</td>\n",
              "      <td>1.488930e+11</td>\n",
              "      <td>6.752800e+10</td>\n",
              "      <td>2.499500e+10</td>\n",
              "      <td>2.187400e+10</td>\n",
              "      <td>7.260000e+08</td>\n",
              "      <td>1.024090e+11</td>\n",
              "      <td>7.897500e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>18.641769</td>\n",
              "      <td>2.590000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.513660e+11</td>\n",
              "      <td>4.507800e+10</td>\n",
              "      <td>2.168900e+10</td>\n",
              "      <td>4.713100e+10</td>\n",
              "      <td>3.386000e+10</td>\n",
              "      <td>2.810000</td>\n",
              "      <td>1.91000</td>\n",
              "      <td>6.498800e+10</td>\n",
              "      <td>1.698640e+11</td>\n",
              "      <td>7.052900e+10</td>\n",
              "      <td>3.675500e+10</td>\n",
              "      <td>2.415800e+10</td>\n",
              "      <td>7.920000e+08</td>\n",
              "      <td>1.244740e+11</td>\n",
              "      <td>8.150200e+10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a502791d-3cde-43eb-8f35-7106bfc76359')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a502791d-3cde-43eb-8f35-7106bfc76359 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a502791d-3cde-43eb-8f35-7106bfc76359');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-665a3ed2-1839-40fa-9544-aa6800ed3fa2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-665a3ed2-1839-40fa-9544-aa6800ed3fa2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-665a3ed2-1839-40fa-9544-aa6800ed3fa2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"close_adjusted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2227.6902213130315,\n        \"min\": 0.464957594871521,\n        \"max\": 6307.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.459530156544617,\n          4.497064113616943,\n          6307.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2229.7275140523216,\n        \"min\": 0.0,\n        \"max\": 6307.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0039015776121769465,\n          2.59,\n          0.05724753603834388\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stock Splits\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2229.7584976982,\n        \"min\": 0.0,\n        \"max\": 6307.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0006342159505311559,\n          2.0,\n          0.03561222759437467\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_sources\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2229.5582161687803,\n        \"min\": 0.0,\n        \"max\": 6307.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          6307.0,\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disparity_bps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2229.8612344717776,\n        \"min\": 0.0,\n        \"max\": 6307.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          6307.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94913658242.6383,\n        \"min\": 1859.0,\n        \"max\": 251366000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          205588994620.76385,\n          217067000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CapitalExpenditures\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16171259760.917503,\n        \"min\": 3850.0,\n        \"max\": 45078000000.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3850.0,\n          31179890909.090908\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CashAndCashEquivalentsAtCarryingValue\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8190587881.218745,\n        \"min\": 3850.0,\n        \"max\": 21689000000.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3850.0,\n          17450761038.96104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CurrentAssets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15807745134.612461,\n        \"min\": 1859.0,\n        \"max\": 47131000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          31716238300.161377,\n          31250000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CurrentLiabilities\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12546339404.088682,\n        \"min\": 1859.0,\n        \"max\": 33860000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          27770338891.877354,\n          26225000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EarningsPerShareBasic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 485.2844076851763,\n        \"min\": 0.58,\n        \"max\": 1374.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1.4850582241630277,\n          1.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EarningsPerShareDiluted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124.35107199004649,\n        \"min\": 0.58,\n        \"max\": 353.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.4993201133144476,\n          1.91\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GrossProfit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22185307293.05369,\n        \"min\": 1859.0,\n        \"max\": 64988000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          38706522861.75363,\n          34067000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Liabilities\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 61880628627.0534,\n        \"min\": 1859.0,\n        \"max\": 169864000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          134575286175.3631,\n          138092000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LongTermDebtNoncurrent\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29777951766.417492,\n        \"min\": 3850.0,\n        \"max\": 70529000000.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3850.0,\n          66410986753.24675\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NetIncomeLoss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12964639556.571209,\n        \"min\": 1859.0,\n        \"max\": 36755000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          13928330285.099516,\n          7414000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"OperatingIncomeLoss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10111992864.630667,\n        \"min\": 3850.0,\n        \"max\": 24158000000.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3850.0,\n          21226466493.506493\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ResearchAndDevelopmentExpense\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 446022905.2655146,\n        \"min\": -563000000.0,\n        \"max\": 792000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          438387305.0026896,\n          641000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Revenues\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 43872580057.95412,\n        \"min\": 1859.0,\n        \"max\": 124474000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          84886649273.80312,\n          84638000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"StockholdersEquity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32678066868.347237,\n        \"min\": 1859.0,\n        \"max\": 81502000000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          71995321140.39806,\n          74215000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(johnson)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIvVpCM1l9QN",
        "outputId": "b5e2cf3e-4006-46de-be21-659521a944b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'meta_input': {'timezone': 'UTC', 'trading_calendar': 'NYSE', 'index_type': 'trading_days_NYSE', 'ajuste_preco': 'adjusted_close_dividends_splits'}, 'empresa': {'ticker': 'PBR', 'nome': 'Petroleo Brasileiro S.A. - Petrobras', 'setor': 'Energy'}, 'alvo': {'variavel': 'preco_acao', 'tipo_alvo': 'preco', 'unidade': 'USD', 'frequencia': 'diaria', 'coluna_valor': 'close_adjusted'}, 'variaveis': {'kpis': [{'nome': 'Assets', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CapitalExpenditures', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CashAndCashEquivalentsAtCarryingValue', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CurrentAssets', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'CurrentLiabilities', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'EarningsPerShareBasic', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'EarningsPerShareDiluted', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'GrossProfit', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'Liabilities', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'LongTermDebtNoncurrent', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'NetIncomeLoss', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'OperatingIncomeLoss', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'ResearchAndDevelopmentExpense', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'Revenues', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}, {'nome': 'StockholdersEquity', 'frequencia': 'diaria (lock_to_publication)', 'fonte': 'SEC companyfacts'}]}, 'operacao': {'coleta_em': '2025-09-09T21:44:31+00:00', 'datas': {'min': '2000-08-10', 'max': '2025-09-09'}, 'linhas': 6307, 'colunas': ['date', 'ticker', 'close_adjusted', 'Dividends', 'Stock Splits', 'n_sources', 'disparity_bps', 'flag_disparity', 'flag_jump', 'Assets', 'CapitalExpenditures', 'CashAndCashEquivalentsAtCarryingValue', 'CurrentAssets', 'CurrentLiabilities', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'Liabilities', 'LongTermDebtNoncurrent', 'NetIncomeLoss', 'OperatingIncomeLoss', 'ResearchAndDevelopmentExpense', 'Revenues', 'StockholdersEquity', 'is_report_Assets', 'is_report_CapitalExpenditures', 'is_report_CashAndCashEquivalentsAtCarryingValue', 'is_report_CurrentAssets', 'is_report_CurrentLiabilities', 'is_report_EarningsPerShareBasic', 'is_report_EarningsPerShareDiluted', 'is_report_GrossProfit', 'is_report_Liabilities', 'is_report_LongTermDebtNoncurrent', 'is_report_NetIncomeLoss', 'is_report_OperatingIncomeLoss', 'is_report_ResearchAndDevelopmentExpense', 'is_report_Revenues', 'is_report_StockholdersEquity']}, 'qa_flags': {'anomalias_jump': 0}, 'proveniencia_opcional': {'checksum_prices_csv_md5': '479318a65769253772cb8b5821643ef0', 'checksum_dataset_csv_md5': '1639447a1e30bd0e8e0b02e814d3a1f9'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a62709f6",
        "outputId": "2a86722d-6013-42eb-94d7-abc0ad6050b2"
      },
      "source": [
        "\n",
        "# Salve o DataFrame em um arquivo CSV\n",
        "df.to_csv(\"pesquisa_daily_kpis_df.csv\", index=False)\n",
        "\n",
        "# Salve o JSON em um arquivo JSON\n",
        "import json\n",
        "with open(\"pesquisa_daily_kpis_johnson.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(johnson, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Arquivos 'pesquisa_daily_kpis_df.csv' e 'pesquisa_daily_kpis_johnson.json' salvos com sucesso.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos 'pesquisa_daily_kpis_df.csv' e 'pesquisa_daily_kpis_johnson.json' salvos com sucesso.\n"
          ]
        }
      ]
    }
  ]
}